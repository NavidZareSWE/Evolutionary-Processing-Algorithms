import { Slide } from '../types';

// Complete slide data for all 248 slides organized by session
export const SLIDES: Slide[] = [
  // ==================== SESSION 1: Introduction (Slides 1-10) ====================
  { number: 1, title: "Introduction - Title Slide", session: 1, content: "Introduction\nBy: Dr. Ali Hamzeh\nShiraz University, CSE and IT Dept.\nJune 2013 Soft Computing Group", explanation: "Title slide for the Evolutionary Computing course taught by Dr. Ali Hamzeh at Shiraz University as part of the Soft Computing research group. Soft Computing encompasses techniques dealing with imprecision and uncertainty including Fuzzy Logic, Neural Networks, and Evolutionary Computation.", keyPoints: ["Course is part of Soft Computing curriculum", "Taught at Shiraz University, Iran", "Focus on Evolutionary Computing"] },
  { number: 2, title: "Session #1", session: 1, content: "Session #1", explanation: "Session 1 covers foundational concepts: What is Intelligence (adaptive capability), Russell's four AI approaches, Intelligent Agents, and AI as Optimization with human-created vs nature-inspired approaches.", keyPoints: ["Foundation for understanding EC", "Connects intelligence, adaptation, evolution", "Sets up course motivation"] },
  { number: 3, title: "Introduction Section", session: 1, content: "Introduction", explanation: "This section establishes fundamental concepts underlying Evolutionary Computing. The key question: What makes something intelligent? The adaptive definition directly motivates evolutionary approaches.", keyPoints: ["Definition of intelligence determines AI approach", "Adaptive view motivates evolutionary computation"] },
  { number: 4, title: "What is Intelligent?", session: 1, content: "What is Intelligent?", explanation: "Different definitions lead to different AI approaches: Intelligence as Knowledge (expert systems), as Computation (faster computers), as Reasoning (logic-based AI), as Learning (ML), or as Adaptation (EC). The adaptive view is operational, general, achievable, and connects to biology.", keyPoints: ["Multiple definitions exist", "Definition affects AI approach", "Course adopts adaptive view"] },
  { number: 5, title: "Intelligence Definition", session: 1, content: "Intelligence can be defined as the capability of a system to adapt its behaviour to ever-changing environment.\n\nRussell's Four Approaches:\n- Think Humanly\n- Act Humanly  \n- Think Rationally\n- Act Rationally\n\nActing rationally leads to Minimize Error", explanation: "Core definition: 'The capability of a system to adapt its behaviour to ever-changing environment.' Each word matters: 'capability' (potential), 'system' (not limited to biological), 'adapt' (directed change), 'behaviour' (external actions), 'ever-changing' (dynamic). Russell's key insight: acting rationally means minimizing error = optimization. Therefore evolutionary algorithms can produce intelligent behavior.", keyPoints: ["Intelligence is fundamentally about adapting behavior to changing environments, making it measurable and achievable", "Russell's four AI approaches (Think/Act × Human/Rational) provide different lenses for building intelligent systems", "Acting rationally reduces to minimizing error, which is fundamentally an optimization problem", "This optimization framing directly connects intelligent behavior to evolutionary algorithms and EC techniques"], definitions: ["intelligence"] },
  { number: 6, title: "Intelligent Agent Question", session: 1, content: "What is Intelligent agent?", explanation: "An agent is something that acts. Not all agents are intelligent. What additional properties make an agent intelligent? Answer combines sensing, acting, adapting, and goal-directedness. EA can be viewed as an intelligent agent.", keyPoints: ["Agent = entity that acts", "Not all agents are intelligent", "EA can be viewed as intelligent agent"] },
  { number: 7, title: "Intelligent Agent Definition", session: 1, content: "• An intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators and directs its activity towards achieving goals (i.e. it is rational)\n• Intelligent agents may also learn or use knowledge\n• They may be very simple or very complex: a thermostat is an intelligent agent, as is a human being, as is a community working together", explanation: "Three components: (1) Autonomous entity that senses and acts, (2) Goal-directed (rational), (3) May learn or use knowledge. Complexity ranges from thermostat to human to communities. An EA POPULATION is like a community of solutions working toward optimization.", keyPoints: ["IA = autonomous + senses + acts + goal-directed", "Can learn and use knowledge", "Ranges from thermostat to communities", "EA is like community of solutions"], definitions: ["intelligentAgent"] },
  { number: 8, title: "AI Question", session: 1, content: "What is Artificial Intelligence?", explanation: "AI coined by John McCarthy in 1956. Has meant different things: symbolic reasoning, expert systems, ML, deep learning. Constant theme is OPTIMIZATION. This course takes the view that AI is fundamentally about optimization.", keyPoints: ["AI coined 1956", "Definition evolved over decades", "Constant theme: OPTIMIZATION"] },
  { number: 9, title: "AI as Optimization", session: 1, content: "In Artificial intelligence we will learn optimization approaches\n\nOptimization approach: Finding the min/max of the function\n\nOptimization approaches classified to:\n- Human created\n- Inspired by nature", explanation: "AI framed as OPTIMIZATION. Most AI problems fit: classification (minimize error), clustering (minimize distance), planning (minimize cost), neural networks (minimize loss), EA (maximize fitness). Two categories: Human-created (calculus-based, LP—have proofs but require structure) and Nature-inspired (EA, NN, swarm—few assumptions, work broadly).", keyPoints: ["AI is fundamentally optimization", "Most AI = min/max of functions", "Human-created vs nature-inspired", "Nature-inspired methods work broadly"] },
  { number: 10, title: "AI Topics", session: 1, content: "In AI we will learn:\n- Fuzzy Logic (FL)\n- Pattern Recognition\n- Evolutionary Computation (EC)\n- Neural Network (NN)\n- Machine Learning (ML)\n- Image processing\n- Vision", explanation: "Major AI topics showing where EC fits. Fields interconnect: EC+NN=Neuroevolution, EC+FL=Evolutionary Fuzzy. EC is a META-TECHNIQUE that can optimize other AI systems.", keyPoints: ["EC is major AI paradigm", "Fields deeply interconnect", "EC can enhance other AI methods"] },
  
  // ==================== SESSION 2: Bio-Inspired Computing (Slides 11-19) ====================
  { number: 11, title: "Session #2", session: 2, content: "Session #2", explanation: "Session 2 introduces biological foundations: Bio-Inspired Computing, Evolutionary Computation definition, Natural Selection mechanism, Three Fundamental Operators (crossover, mutation, selection), Evolution vs Adaptation, why human evolution stopped.", keyPoints: ["Bridges biology and CS", "Introduces three operators", "Establishes biological foundation"] },
  { number: 12, title: "Bio-Inspired Computing", session: 2, content: "Bio-inspired computing is the use of computers to model living phenomena, and simultaneously the study of life to improve computers.\n\nRelies on biology, computer science, mathematics.\n\nBranches: NN, EC, ...", explanation: "TWO-WAY relationship: computers model life AND life improves computers. Relies on Biology (mechanisms), CS (algorithms), Mathematics (theory). Main branches: Neural Networks (brain structure), Evolutionary Computation (evolution), plus Swarm, Immune Systems. We're INSPIRED by biology, not copying exactly.", keyPoints: ["Two-way relationship", "Relies on biology, CS, math", "Main branches: NN, EC, Swarm", "Bio-INSPIRED not bio-mimicking"] },
  { number: 13, title: "Evolutionary Computation", session: 2, content: "Evolutionary Computation:\nThe evolutionary approach is based on computational models of natural selection and genetics.", explanation: "EC = computational models of NATURAL SELECTION + GENETICS. Not exact simulations. Natural selection: variation, survival advantage, inheritance. Genetics: encoding (genotype), expression (phenotype), inheritance, variation. EC characteristics: Population-based, Stochastic, Black-box, Iterative.", keyPoints: ["EC combines computational models of natural selection (survival of fittest) with genetics (inheritance and variation)", "We are inspired by biology's principles, not attempting to exactly replicate biological mechanisms", "Core characteristics include being population-based, stochastic, black-box applicable, and iterative in nature", "The genotype-phenotype distinction separates how solutions are encoded from how they are evaluated"], definitions: ["fitness", "genotype", "phenotype"] },
  { number: 14, title: "Natural Selection", session: 2, content: "Limited resources → Competition → Success depends on fitness\n\nFitness: how well-adapted to environment, determined by genes\n\nSuccessful individuals reproduce and pass on genes\n\nWhen changes occur, previously fit individuals may no longer be best-suited", explanation: "Natural selection prerequisites: Limited resources (not all survive), Competition (selection pressure), Fitness determines success. CRITICAL: We only know CORRECT fitness (ranking) NOT EXACT fitness (values). This is why FPS is problematic while tournament selection is robust. Inheritance allows improvement. Environmental change requires variation.", keyPoints: ["Limited resources → competition → selection", "Fitness = adaptation to environment", "CRITICAL: Know CORRECT not EXACT fitness", "Inheritance enables improvement"], definitions: ["fitness"], professorNote: "We only know CORRECT fitness (ranking), not EXACT fitness (values). This is why FPS is problematic." },
  { number: 15, title: "Three Operators", session: 2, content: "[Triangle diagram]\n• X-over (top)\n• mutation (bottom left)\n• selection (bottom right)", explanation: "THREE OPERATORS drive all genetic change: CROSSOVER: Combines genetic material, cannot create new alleles, large steps, EXPLORES. MUTATION: Creates new material, introduces new alleles, small steps, EXPLOITS. SELECTION: Guides toward fitness, doesn't change material, provides direction. All three needed: Selection alone—no new solutions. Mutation alone—random. Crossover alone—no new alleles.", keyPoints: ["Three operators: Crossover, Mutation, Selection", "Crossover COMBINES (large steps)", "Mutation CREATES (small steps)", "Selection GUIDES (direction)", "All three needed"], definitions: ["crossover", "mutation", "selection"] },
  { number: 16, title: "Evolution vs Adaptation", session: 2, content: "Evolution creates optimization procedures\nEvolution has optimized biological processes\nAdoption of evolutionary paradigm helps find optimal solutions\nAdaptation leads individuals to adapt to environment", explanation: "EVOLUTION = process on populations over generations (methodology). ADAPTATION = outcome where individuals fit environment (result). Evolution ENABLES adaptation. Adaptation DRIVES evolution. For EA: apply evolutionary PROCESSES to achieve solution ADAPTATION.", keyPoints: ["Evolution = process/methodology", "Adaptation = outcome", "Evolution enables adaptation", "EA applies process for adaptation"] },
  { number: 17, title: "Darwinian Theory", session: 2, content: "Darwin's theory offers explanation of biological diversity\n\nGiven limited resources and instinct to reproduce, selection becomes inevitable if population size is not to grow exponentially", explanation: "Darwin provided NATURAL mechanism for diversity. Mathematical argument: With limited capacity K and reproduction rate R>1, without selection: exponential growth N(g)=N(0)×R^g. But environment has capacity K. When N>K, selection is MATHEMATICALLY INEVITABLE. This justifies selection in EA.", keyPoints: ["Darwin: natural mechanism for diversity", "Selection mathematically inevitable", "Exponential growth impossible with limits", "Justifies selection in EA"] },
  { number: 18, title: "Human Brain", session: 2, content: "Brain is the best solution finder in nature\n\nApproaches: NN (Strong optimizer), EC\n\nEvolutionary approaches use: mutation, X-over, selection\n\nAny approach using these three operators is bio-inspired", explanation: "Brain is nature's best optimizer. Two approaches: NN (brain STRUCTURE) and EC (brain's evolutionary ORIGIN). These complement each other: Neuroevolution. Any approach using mutation + crossover + selection is bio-inspired.", keyPoints: ["Brain is best optimizer", "NN: structure, EC: evolutionary origin", "They complement each other", "Three operators define bio-inspired"] },
  { number: 19, title: "Why Evolution Stopped", session: 2, content: "X-over is stopped\nselection is stopped\nmutation is stopped\n\nEvolution stopped while we stop these three operators", explanation: "Human evolution slowed because all three operators weakened: CROSSOVER: Partner choice not fitness-based. SELECTION: Medicine/welfare remove selection pressure. MUTATION: Still occurs but no directional effect without selection. KEY: Evolution requires ALL THREE. For EA: maintain all three, don't disable any.", keyPoints: ["All three operators weakened", "X-over: not fitness-based", "Selection: removed by society", "Mutation: no effect without selection", "EA needs all three"] },
  
  // ==================== SESSION 3: Four EA Brands & Framework (Slides 20-31) ====================
  { number: 20, title: "Session #3", session: 3, content: "Session #3", explanation: "Session 3: Four EA families (GA, ES, GP, EP), EC position in science, Evolution-Problem mapping, Why use evolution, General EA Scheme, Phenotype vs Genotype.", keyPoints: ["Four EA families", "Universal framework", "Key vocabulary"] },
  { number: 21, title: "Four EA Brands", session: 3, content: "• GA (Genetic Algorithm)\n• ES (Evolutionary Strategy)\n• GP (Genetic Programming)\n• EP (Evolutionary Programming)", explanation: "Four families: GA - USA 1970s, binary, crossover emphasis, FPS. ES - Germany 1960s, real vectors, mutation emphasis, self-adaptive. GP - USA 1990s, trees, evolving programs. EP - USA 1960s, FSM, mutation only, tournament. Modern view: distinctions largely irrelevant, mix approaches freely.", keyPoints: ["GA: binary, crossover, USA 1970s", "ES: real, mutation, Germany 1960s", "GP: trees, programs, 1990s", "EP: FSM, mutation only, 1960s", "Modern: mix approaches"] },
  { number: 22, title: "EC Position", session: 3, content: "science → Exact science → Computer science → Computational intelligence → Evolutionary computing", explanation: "EC position: Science → Exact Sciences → CS → Computational Intelligence → EC. EC inherits from CS: algorithm analysis, complexity. From CI: nature-inspired philosophy, adaptation focus.", keyPoints: ["EC in CI in CS", "Part of exact sciences", "Related to NN, Fuzzy", "Inherits from hierarchy"] },
  { number: 23, title: "Evolution vs Problem Solving", session: 3, content: "Evolution ↔ Problem solving\nEnvironment ↔ problem\nindividual ↔ Candidate solution\nfitness ↔ Quality", explanation: "Fundamental MAPPING: Environment↔Problem (defines what's good), Individual↔Solution (one answer), Fitness↔Quality (how good). Extended: Population↔Solution set, Generation↔Iteration, Gene↔Variable, Mutation↔Random variation, Crossover↔Combination.", keyPoints: ["Environment ↔ Problem", "Individual ↔ Solution", "Fitness ↔ Quality", "Enables transfer from biology"] },
  { number: 24, title: "Why Evolution Part 1", session: 3, content: "• Problems require search through many possibilities\n• Search space too big\n• Better solved using parallel approach\n• Evolution proves good model", explanation: "Why EA: (1) Problems require SEARCH through huge spaces. (2) Spaces TOO BIG—can't enumerate (2^100 ≈ 10^30). (3) PARALLEL search covers more space. (4) Evolution WORKS—billions of years evidence.", keyPoints: ["Problems require search", "Spaces exponentially large", "Parallel search helps", "Evolution proven effective"] },
  { number: 25, title: "Why Evolution Part 2", session: 3, content: "• Evolution searches for optimal from many possibilities\n• Evolution is parallel process\n• Evolution designs new solutions to changing environment", explanation: "Evolution as method: SEARCH for optimal, PARALLEL (N solutions simultaneously), DESIGNS new solutions (variation creates novelty), handles CHANGING environments. Synthesis: Search + Parallelism + Creativity + Adaptability.", keyPoints: ["Search method", "Parallel: N simultaneous", "Creative: novel solutions", "Adaptive: handles change"] },
  { number: 26, title: "General EA Scheme", session: 3, content: "initialization → population → Parent selection → parents → Recombination/mutation → offspring → Survival selection → [loop] → termination", explanation: "UNIVERSAL EA FRAMEWORK: (1) INITIALIZATION: Create N random solutions. (2) PARENT SELECTION: Choose by fitness. (3) VARIATION: Crossover + mutation. (4) SURVIVOR SELECTION: Who continues. (5) TERMINATION: Target, generations, stagnation. All EA variants fit this.", keyPoints: ["Universal: Init → [Select → Vary → Survive] → Terminate", "All EAs follow this", "Components modular", "Parent vs Survivor selection"] },
  { number: 27, title: "EA Components", session: 3, content: "• Representation\n• Evaluation function\n• Population\n• Parent selection\n• Variation operators\n• Survivor selection", explanation: "Six components: REPRESENTATION (how encoded), FITNESS (measures quality), POPULATION (set of solutions), PARENT SELECTION (who reproduces), VARIATION (crossover, mutation), SURVIVOR SELECTION (who continues). Each independent—this modularity is EA's strength.", keyPoints: ["Six components", "Representation matches problem", "Fitness IS the problem", "Components independent"] },
  { number: 28, title: "EA Behavior", session: 3, content: "[Graph: fitness over generations]\nBest increases rapidly then slows\nAverage follows, gap narrows", explanation: "Typical behavior: EARLY: Rapid improvement, large variance. MIDDLE: Slows, converges. LATE: Diminishing returns, fine-tuning. Best never decreases (with elitism). Stagnation indicates parameter problems.", keyPoints: ["Rapid early improvement", "Best increases, average follows", "Variance decreases", "Stagnation = problems"] },
  { number: 29, title: "Representation & Variation", session: 3, content: "Representation and variation operators together specify the neighborhoods in the fitness landscape that EA can explore", explanation: "CRITICAL: Representation + Variation define what EA can explore. Representation = space of solutions. Variation = neighborhood structure. Mismatched operators can make good solutions unreachable. Representation is FIRST and MOST IMPORTANT decision.", keyPoints: ["Rep + Variation define landscape", "Rep = solution space", "Variation = neighborhoods", "Mismatch prevents finding optima"] },
  { number: 30, title: "Phenotype vs Genotype", session: 3, content: "Phenotype = external appearance\nGenotype = internal data structure\n\nDecoding = Genotype → Phenotype\nEncoding = Phenotype → Genotype", explanation: "GENOTYPE = encoding we manipulate (binary, real, tree). PHENOTYPE = decoded solution we evaluate. DECODING: Genotype → Phenotype. Can be identical (GP) or different (binary→real). Fitness evaluates PHENOTYPE, operators modify GENOTYPE.", keyPoints: ["Genotype = encoding", "Phenotype = solution", "Decoding: G → P", "Fitness on phenotype, operators on genotype"], definitions: ["genotype", "phenotype"] },
  { number: 31, title: "Genotype-Phenotype Mapping", session: 3, content: "Decoding maps genotype to phenotype\nNeeded for fitness calculation\nMay be complex", explanation: "DECODING: Simple (binary→integer), Complex (grammar→program), Identity (tree IS program). Should be surjective (all phenotypes reachable). Poor mapping prevents finding good solutions.", keyPoints: ["Decoding maps G to P", "Can be simple/complex/identity", "Should be surjective", "Poor mapping limits search"] },
  
  // ==================== SESSION 4: Fitness & Population (Slides 32-47) ====================
  { number: 32, title: "Session #4", session: 4, content: "Session #4", explanation: "Session 4: Fitness function definition, Quality and Discrimination, Diversity hierarchy, Parent vs Survivor selection, Mutation vs Crossover debate.", keyPoints: ["Fitness defines problem", "Diversity hierarchy crucial", "Crossover explores, mutation exploits"] },
  { number: 33, title: "Fitness Function", session: 4, content: "Represents requirements population should adapt to\na.k.a. quality/objective function\n\nAssigns single real-valued fitness to each phenotype\n\nMore discrimination the better\n\nTypically maximize (minimize converts trivially)", explanation: "FITNESS FUNCTION most important—DEFINES THE PROBLEM. Input: Phenotype. Output: Single real number. DISCRIMINATION: more different values = better guidance. PROFESSOR'S POINT: We only know CORRECT fitness (ranking) not EXACT (values). FPS uses exact (problematic), Tournament uses only comparisons (robust).", keyPoints: ["Fitness DEFINES problem", "Input: Phenotype; Output: real number", "More discrimination = better", "Know CORRECT not EXACT fitness"], definitions: ["fitness"], professorNote: "We only know CORRECT fitness (ranking), not EXACT fitness (values)." },
  { number: 34, title: "Population Role", session: 4, content: "Population holds candidate solutions\n\nSelection: population level\nVariation: individual level\n\nDiversity crucial", explanation: "POPULATION holds solutions. Selection at POPULATION level, Variation at INDIVIDUAL level. DIVERSITY crucial—without it, selection has nothing to choose, crossover can't combine different solutions.", keyPoints: ["Population holds solutions", "Selection: population; Variation: individual", "Diversity crucial"] },
  { number: 35, title: "Diversity Hierarchy", session: 4, content: "Fitness diversity → Phenotype Diversity → Genotype Diversity\n\nEach implies next but NOT reverse", explanation: "CRITICAL: Fitness→Phenotype→Genotype diversity. Each implies next but NOT reverse. Different genotypes may encode same phenotype. Different phenotypes may have same fitness. Fitness diversity matters for selection.", keyPoints: ["Hierarchy: Fitness → Phenotype → Genotype", "Each implies next, NOT reverse", "Different genotypes can = same phenotype", "Fitness diversity matters"], definitions: ["diversityHierarchy"] },
  { number: 36, title: "Parent Selection", session: 4, content: "Distinguish individuals by quality\nHigher probability to fitter\nPermits 'bad' individuals some chance (diversity)", explanation: "PARENT SELECTION chooses reproducers. Higher probability to fitter. BUT allow bad individuals some chance—they may have needed building blocks. Without this, those alleles lost forever.", keyPoints: ["Higher probability to fitter", "Bad individuals get some chance", "May contain needed building blocks", "Loss of diversity is failure mode"] },
  { number: 37, title: "Survivor Selection", session: 4, content: "Distinguish by quality\nSometimes deterministic (elitism)\nParent + Survivor = selection pressure", explanation: "SURVIVOR SELECTION chooses who continues. Often deterministic (elitism). Parent + Survivor = total selection pressure. Too high: premature convergence. Too low: random search.", keyPoints: ["Chooses who continues", "Often deterministic (elitism)", "Total pressure = parent + survivor", "Balance for appropriate pressure"] },
  { number: 38, title: "Variation Operators", session: 4, content: "Generate new candidates\n\nMutation: acts on one\nRecombination: acts on two+", explanation: "VARIATION generates new solutions. MUTATION (unary): creates new material, small changes. RECOMBINATION (binary): combines existing, larger changes. Both stochastic. Must match representation.", keyPoints: ["Generate new solutions", "Mutation: unary, new material", "Recombination: binary, combines", "Match to representation"] },
  { number: 39, title: "Recombination Role", session: 4, content: "Merges parent information\nChoice of what to merge is stochastic\nHope: combine useful building blocks", explanation: "RECOMBINATION merges parents: Combines parts, stochastic choice. Hope: combine useful building blocks. CANNOT create new alleles—only combines existing. EXPLORATIVE—large steps.", keyPoints: ["Merges parent information", "Hope: combine building blocks", "Cannot create new alleles", "Explorative—large steps"], definitions: ["crossover"] },
  { number: 40, title: "Mutation Role", session: 4, content: "Acts on one genotype\nDelivers slightly modified mutant\nRandomness essential to escape local optima", explanation: "MUTATION modifies single genotype. Creates NEW material—introduces alleles not in population. Provides RANDOMNESS to escape local optima. ONLY mechanism for new alleles. EXPLOITATIVE—local search.", keyPoints: ["Modifies one genotype", "ONLY creates new alleles", "Escape local optima", "Exploitative—local search"], definitions: ["mutation"] },
  { number: 41, title: "Crossover vs Mutation Debate", session: 4, content: "GA: crossover important, mutation repairs\nES/EP: mutation primary, crossover optional\n\nResolution: Different roles", explanation: "Historical debate: GA emphasized crossover, ES/EP emphasized mutation. RESOLUTION: Different roles. Crossover = EXPLORATIVE (large steps, combines). Mutation = EXPLOITATIVE (small steps, creates new). BOTH needed.", keyPoints: ["GA: crossover; ES/EP: mutation", "Resolution: different roles", "Crossover = EXPLORATIVE", "Mutation = EXPLOITATIVE"] },
  { number: 42, title: "Exploration vs Exploitation", session: 4, content: "Crossover explorative: large step bounded by parents\n\nMutation exploitative: small modifications around current", explanation: "CROSSOVER EXPLORATIVE: Large steps, offspring 'between' parents, explores combinations. MUTATION EXPLOITATIVE: Small steps, fine-tunes, local variation. Opposite to some intuitions! Balance required.", keyPoints: ["Crossover: large steps, between parents", "Mutation: small steps, local", "Opposite to intuition", "Need balance"] },
  { number: 43, title: "Why Mutation Cannot Be Zero", session: 4, content: "Crossover only mixes, cannot create new material\nMutation creates new material\nOnly mutation ensures connectivity (ergodicity)", explanation: "CRITICAL: Mutation cannot be zero. Crossover ONLY mixes—cannot create alleles not in population. If allele lost, crossover cannot recover. Only MUTATION introduces missing alleles. ERGODICITY: any solution reachable only with mutation.", keyPoints: ["Crossover cannot create new alleles", "Mutation creates new alleles", "Only mutation ensures ergodicity", "Zero mutation = trapped"] },
  { number: 44, title: "EA Without Crossover", session: 4, content: "EA with mutation+selection can work (ES, EP)\nEA with crossover only cannot work (no new material)", explanation: "ASYMMETRY: Mutation-only CAN work (ES, EP do this). Crossover-only CANNOT work—no new alleles. Mutation more fundamental, but best uses BOTH.", keyPoints: ["Mutation-only can work", "Crossover-only cannot", "Mutation more fundamental", "Best uses both"] },
  { number: 45, title: "Initialization", session: 4, content: "Usually random, uniform distribution\nSeed with known good solutions if available\nEnsure diversity", explanation: "INITIALIZATION: Usually RANDOM for coverage. Can SEED with known solutions but maintain diversity. Poor initialization limits entire search.", keyPoints: ["Usually random", "Can seed with known solutions", "Ensure diversity", "Poor init limits search"] },
  { number: 46, title: "Termination", session: 4, content: "Reaching optimum/target\nMax time/generations/evaluations\nFitness stagnation", explanation: "When to STOP: Target reached, Resource limits (generations, evaluations, time), Stagnation (no improvement). EA is anytime—always has best-so-far.", keyPoints: ["Target reached", "Resources exhausted", "Stagnation", "Anytime algorithm"] },
  { number: 47, title: "EA Design Guidelines", session: 4, content: "1. Representation for problem\n2. Fitness captures objectives\n3. Operators for representation\n4. Selection balances explore/exploit\n5. Parameters tuned", explanation: "Design priority: (1) REPRESENTATION—most important. (2) FITNESS—defines problem. (3) OPERATORS—match representation. (4) SELECTION—balance explore/exploit. (5) PARAMETERS—require tuning.", keyPoints: ["Representation most important", "Fitness matches objectives", "Operators match representation", "Selection balances", "Parameters need tuning"] },
  
  // ==================== SESSION 5: 8-Queens (Slides 48-55) ====================
  { number: 48, title: "Session #5", session: 5, content: "Session #5: 8-Queens Example", explanation: "Complete walkthrough of 8-Queens using EA. Demonstrates permutation representation, fitness design, operators, full evolutionary cycle.", keyPoints: ["Complete EA example", "Permutation representation", "Full cycle demonstration"] },
  { number: 49, title: "8-Queens Definition", session: 5, content: "Place 8 queens on 8×8 board, no attacks\n\nQueens attack horizontally, vertically, diagonally\n\nClassic constraint satisfaction", explanation: "8-QUEENS: Place 8 queens, no attacks. Naive: 64^8 ≈ 2.8×10^14. One per row: 8^8 ≈ 16.8M. Permutation: 8! = 40,320. Permutation MASSIVELY reduces space.", keyPoints: ["Place 8 queens, no attacks", "Classic constraint problem", "Permutation reduces 64^8 to 8!"] },
  { number: 50, title: "Permutation Representation", session: 5, content: "Permutation of [1,2,3,4,5,6,7,8]\nPosition i = column of queen in row i\n\n[3,1,6,2,8,5,7,4] means:\nRow 1: Column 3\nRow 2: Column 1...", explanation: "Chromosome = permutation of [1-8]. Position i = column in row i. AUTOMATICALLY satisfies: one queen per row (array structure), one per column (permutation property). Only check DIAGONALS.", keyPoints: ["Permutation gives column per row", "Auto satisfies row/column", "Only diagonal conflicts", "Reduces to 8! = 40,320"] },
  { number: 51, title: "8-Queens Fitness", session: 5, content: "Fitness = 28 - (attacking pairs)\n\nC(8,2) = 28 max pairs\n\nPerfect = 28 (no attacks)", explanation: "Fitness = 28 - (attacking pairs). C(8,2) = 28 pairs total. Perfect: 0 attacks → Fitness 28. Good discrimination—fewer conflicts = higher fitness. Easy O(n²) to compute.", keyPoints: ["Fitness = 28 - attacks", "28 = max non-attacking pairs", "Perfect solution = 28"], formulas: ["eightQueensFitness"] },
  { number: 52, title: "8-Queens Crossover", session: 5, content: "Use permutation crossover:\n- Order-based\n- PMX\n\nMust preserve permutation property", explanation: "Crossover must preserve permutation (each value once). Standard crossover FAILS (duplicates). Use: Order-1, PMX, Cycle. For 8-Queens, position matters → PMX natural.", keyPoints: ["Standard crossover fails", "Use Order-1, PMX, Cycle", "PMX preserves positions"] },
  { number: 53, title: "8-Queens Mutation", session: 5, content: "Mutation for permutations:\n- Swap: exchange two positions\n- Insert: move element\n- Inversion: reverse substring\n\nAll preserve permutation", explanation: "Swap: Exchange two values. Insert: Move element. Inversion: Reverse substring. All preserve permutation property. Swap most common for simplicity.", keyPoints: ["Swap: exchange two", "Insert: move element", "Inversion: reverse", "All preserve permutation"] },
  { number: 54, title: "8-Queens Walkthrough", session: 5, content: "Initial: random, avg fitness ~24\nGeneration 10: ~26\nGeneration 50: 28 (solution)\n\nTypically 50-100 generations", explanation: "Initial random: fitness ~24. Rapid improvement to ~26. Solution (28) usually in 50-100 generations. 10,000 evaluations vs 40,320 random—much faster.", keyPoints: ["Initial ~24", "Solution in 50-100 generations", "Much faster than random"] },
  { number: 55, title: "8-Queens Lessons", session: 5, content: "1. Representation matters (permutation eliminates conflicts)\n2. Fitness provides gradient\n3. Operators match representation\n4. EA finds solutions efficiently", explanation: "Lessons: (1) Representation eliminated constraints. (2) Fitness guided search. (3) Operators matched permutation. (4) EA efficient in large space.", keyPoints: ["Representation eliminated constraints", "Fitness provided gradient", "Operators matched", "EA efficient"] },
  
  // ==================== SESSION 6: Simple GA (Slides 56-70) ====================
  { number: 56, title: "Session #6", session: 6, content: "Session #6: Simple GA", explanation: "SGA: binary representation, FPS, one-point crossover, bit-flip mutation, generational replacement. Gray coding.", keyPoints: ["SGA canonical form", "Binary with crossover emphasis", "Roulette wheel selection"] },
  { number: 57, title: "SGA Overview", session: 6, content: "SGA Components:\n- Binary representation\n- Fitness-proportionate selection\n- One-point crossover\n- Bit-flip mutation\n- Generational replacement", explanation: "SGA by Goldberg (1989): Binary strings, FPS (roulette wheel), one-point crossover (0.6-0.9), bit-flip mutation (0.001-0.01), generational replacement. Textbook GA for understanding theory.", keyPoints: ["Binary representation", "FPS selection", "One-point crossover", "Bit-flip mutation", "Generational replacement"] },
  { number: 58, title: "Binary Representation", session: 6, content: "Solutions as binary strings: 10110101\n\nEncode: integers, reals, multiple parameters", explanation: "Binary encodes: Integers (direct), Reals (scaled), Multiple parameters (concatenated). Universal but may not match problem. Precision limited by bits.", keyPoints: ["Encode as binary", "Integers, reals, multi-param", "Universal but may mismatch"] },
  { number: 59, title: "Binary to Real Decoding", session: 6, content: "x = a + (b-a) × value/(2^l - 1)\n\n5 bits for [0,10]:\n10110 = 22 → x = 7.10", explanation: "Formula: x = a + (b-a) × v/(2^l-1). Example: 5 bits, [0,10], 10110=22 → x = 10×22/31 = 7.10. Precision = (b-a)/(2^l-1).", keyPoints: ["x = a + (b-a) × v/(2^l-1)", "More bits = more precision"] },
  { number: 60, title: "Goldberg x² Example", session: 6, content: "Maximize f(x) = x²\nx ∈ [0,31], 5-bit binary\n\nMax at x=31 (11111), f=961", explanation: "Classic: Maximize x² on [0,31]. 5-bit: 00000-11111. Max at 31 (11111), f=961. Simple unimodal, demonstrates GA basics.", keyPoints: ["Maximize x² on [0,31]", "5-bit binary", "Max at 11111 (31)"] },
  { number: 61, title: "One-Point Crossover", session: 6, content: "Choose random cut point\nSwap tails\n\n11001|010 + 00110|101\n→ 11001|101 + 00110|010", explanation: "One-point: Random cut (l-1 positions), swap tails. Has POSITIONAL BIAS: nearby genes stay together, opposite ends NEVER together. Pc ≈ 0.6-0.9.", keyPoints: ["Random cut, swap tails", "Positional bias", "Nearby stay together", "Pc ≈ 0.6-0.9"], definitions: ["positionalBias"] },
  { number: 62, title: "N-Point and Uniform", session: 6, content: "N-point: multiple cuts, alternate segments\n\nUniform: each bit from random parent\n\nUniform: no positional bias, distributional bias toward 50%", explanation: "N-point: Multiple cuts, alternate segments. Uniform: Each bit independently from random parent. Uniform has no positional bias but 50-50 distributional bias.", keyPoints: ["N-point: multiple cuts", "Uniform: each bit random", "Uniform: no positional bias", "Uniform: 50-50 distribution"], definitions: ["distributionalBias"] },
  { number: 63, title: "Bit-Flip Mutation", session: 6, content: "Flip each bit with probability pm\n\npm = 1/l typical (one bit per chromosome)\n\n11001010 → 11011010", explanation: "Flip each bit with pm. Typical: pm = 1/l (one flip expected). Too high destroys solutions, too low limits variation. Only source of new alleles.", keyPoints: ["Flip with probability pm", "pm = 1/l typical", "Only source of new alleles"] },
  { number: 64, title: "Roulette Wheel Selection", session: 6, content: "P(select i) = f(i) / Σf(j)\n\nfitness(A)=3 → 50%\nfitness(B)=1 → 17%\nfitness(C)=2 → 33%", explanation: "FPS: P(i) = f(i)/Σf(j). Roulette wheel: slices sized by fitness. PROBLEMS: Uses EXACT fitness (we only have CORRECT), scaling sensitivity, premature convergence. Tournament selection addresses all.", keyPoints: ["P(i) = f(i)/Σf(j)", "Slice size = fitness proportion", "Uses exact but have only correct", "Problems: scaling, convergence"], formulas: ["expectedCopiesFPS"], definitions: ["fitnessProportionateSelection"], professorNote: "FPS uses EXACT fitness but we only have CORRECT fitness (rankings)." },
  { number: 65, title: "Expected Copies", session: 6, content: "E(n_i) = μ × f(i)/⟨f⟩\n\nf(i) = ⟨f⟩ → expect 1 copy\nf(i) = 2⟨f⟩ → expect 2 copies", explanation: "Expected copies: E(n_i) = μ × f(i)/⟨f⟩. Twice average → 2 copies. But actual varies significantly (high variance).", keyPoints: ["E(n_i) = μ × f(i)/⟨f⟩", "Twice average → 2 copies", "High variance in actual"], formulas: ["expectedCopiesFPS", "varianceTheorem"] },
  { number: 66, title: "SUS", session: 6, content: "SUS: n evenly spaced pointers, spin once\n\nGuarantees: floor(E) ≤ actual ≤ ceil(E)\n\nLower variance than roulette", explanation: "SUS: N evenly spaced pointers, single spin. Guarantees actual within ±1 of expected. Lower variance but still has FPS problems.", keyPoints: ["N pointers, single spin", "Actual within ±1 of expected", "Lower variance", "Still has FPS problems"] },
  { number: 67, title: "Hamming Cliffs", session: 6, content: "Adjacent integers differ in many bits:\n7 = 0111\n8 = 1000\n\nDiffer in ALL 4 bits!", explanation: "Hamming cliffs: Adjacent numbers differ in many bits. 7(0111) vs 8(1000) differ in ALL bits. Single bit flip makes large change. Solution: Gray coding.", keyPoints: ["Standard binary encoding creates 'cliffs' where adjacent integers (like 7 and 8) differ in many or all bits", "A single bit mutation can cause unexpectedly large phenotypic changes when crossing these cliffs", "This creates a mismatch between small genotype changes and their phenotype effects, harming search", "Gray coding solves this by ensuring adjacent integers always differ by exactly one bit"] },
  { number: 68, title: "Gray Coding", session: 6, content: "Adjacent integers differ by ONE bit\n\n0=0000, 1=0001, 2=0011, 3=0010...\n\nGray = Binary XOR (Binary >> 1)", explanation: "Gray code: Adjacent integers differ by ONE bit. Conversion: Gray = Binary XOR (Binary >> 1). 7 and 8 now differ by one bit. Use for numeric optimization.", keyPoints: ["Adjacent differ by one bit", "Gray = Binary XOR (Binary >> 1)", "Smooth landscape"] },
  { number: 69, title: "SGA Summary", session: 6, content: "SGA Recipe:\n1. Binary (possibly Gray)\n2. FPS (roulette/SUS)\n3. One-point crossover (Pc ≈ 0.7)\n4. Bit-flip mutation (Pm ≈ 1/l)\n5. Generational replacement", explanation: "SGA: Binary strings, FPS selection, one-point crossover Pc≈0.7, bit-flip Pm≈1/l, generational replacement. Classic but modern practice often differs.", keyPoints: ["Binary representation", "FPS selection", "One-point Pc≈0.7", "Bit-flip Pm≈1/l", "Generational"] },
  { number: 70, title: "Session 6 Summary", session: 6, content: "SGA is canonical\nBinary universal but may mismatch\nFPS has problems (addressed later)\nGray coding fixes Hamming cliffs", explanation: "SGA canonical but not always best. Binary universal but may mismatch. FPS has fundamental problems. Gray coding fixes Hamming cliffs.", keyPoints: ["SGA canonical not best", "Binary may mismatch", "FPS has problems", "Gray fixes cliffs"] },
  
  // ==================== SESSION 7: Permutation & Crossover (Slides 71-99) ====================
  { number: 71, title: "Session #7", session: 7, content: "Session #7: Permutation Representations", explanation: "Permutation for ordering problems. Order vs Adjacency. Crossover operators: Order-1, PMX, Cycle, Edge. Positional/distributional bias.", keyPoints: ["Permutation for ordering", "Different crossovers preserve different properties", "Match operator to problem"] },
  { number: 72, title: "Why Permutation", session: 7, content: "Many problems involve orderings:\n- TSP\n- Scheduling\n- Assignment\n\nPermutation encodes 'each item exactly once'", explanation: "Permutation for ordering/assignment: TSP (city order), Scheduling (job order), Assignment (items to slots). Encodes 'each item once' automatically. No invalid solutions.", keyPoints: ["For ordering/assignment", "Encodes 'each once' automatically", "No invalid solutions"] },
  { number: 73, title: "Order vs Adjacency", session: 7, content: "Order-based: relative order matters (scheduling)\n\nAdjacency-based: neighbors matter (TSP)\n\nDifferent crossovers needed", explanation: "ORDER-BASED: Which before which (scheduling). ADJACENCY-BASED: Which next to which (TSP). Different operators: Order-1 preserves order, Edge preserves adjacencies. CRUCIAL distinction.", keyPoints: ["Order-based problems care about relative sequence (which task comes before which in scheduling)", "Adjacency-based problems care about which elements are neighbors (which cities connect in TSP)", "Using the wrong crossover destroys the building blocks your problem actually depends on", "This is perhaps the most critical decision in permutation GA design—match operator to what matters"], definitions: ["positionalBias", "distributionalBias"] },
  { number: 74, title: "Order-1 Crossover", session: 7, content: "Order-1 (OX1):\n1. Copy segment from P1\n2. Fill from P2 in order, skip duplicates\n\nPreserves relative order", explanation: "Order-1: Copy segment from P1, fill rest from P2 in order, skip duplicates. PRESERVES relative order from P2. Good for scheduling.", keyPoints: ["Copy segment from P1", "Fill from P2 in order", "Preserves RELATIVE ORDER"] },
  { number: 75, title: "PMX Crossover", session: 7, content: "PMX:\n1. Copy segment from P1\n2. Fill from P2, use mapping for conflicts\n\nPreserves absolute positions", explanation: "PMX: Copy segment from P1, fill from P2, use mapping for conflicts. PRESERVES absolute positions. Good when positions matter.", keyPoints: ["Copy segment from P1", "Mapping for conflicts", "Preserves ABSOLUTE POSITIONS"] },
  { number: 76, title: "Cycle Crossover", session: 7, content: "Cycle (CX):\n1. Find cycles between parents\n2. Alternate cycles from each\n\nPreserves position-value pairs exactly", explanation: "Cycle: Find cycles between parents, take alternate cycles. PRESERVES position-value pairs exactly. Good when exact positions matter.", keyPoints: ["Find cycles", "Alternate from each parent", "Preserves position-value pairs"] },
  { number: 77, title: "Edge Crossover", session: 7, content: "Edge (ER):\n1. Build edge table from both parents\n2. Construct child following edges\n3. Prefer common edges\n\nPreserves adjacencies - good for TSP", explanation: "Edge: Build edge table, construct child following edges, prefer common edges. PRESERVES adjacencies. Ideal for TSP.", keyPoints: ["Build edge table", "Follow edges", "Prefer common edges", "Preserves ADJACENCIES"] },
  { number: 78, title: "Crossover Comparison", session: 7, content: "Order-1: relative order\nPMX: absolute positions\nCycle: position-value pairs\nEdge: adjacencies\n\nMatch to problem!", explanation: "Order-1: relative order. PMX: absolute positions. Cycle: position-value pairs. Edge: adjacencies. MATCH to problem structure!", keyPoints: ["Order-1: relative order", "PMX: positions", "Cycle: pairs", "Edge: adjacencies", "Match to problem"] },
  { number: 79, title: "Positional Bias", session: 7, content: "1-point: strong positional bias\nGenes at ends NEVER kept together\n\nUniform: no positional bias", explanation: "1-point has strong positional bias. Genes at opposite ends can NEVER stay together. Uniform has no positional bias.", keyPoints: ["1-point: strong bias", "Ends never together", "Uniform: no bias"], definitions: ["positionalBias"] },
  { number: 80, title: "Distributional Bias", session: 7, content: "Uniform: tends toward 50-50\nN-point: depends on cuts\n\nConsider what distribution needed", explanation: "Uniform: 50-50 distributional bias. N-point: varies by cut positions. Consider what distribution makes sense.", keyPoints: ["Uniform: 50-50", "N-point: varies", "Consider needed distribution"], definitions: ["distributionalBias"] },
  { number: 81, title: "Good Crossover Design", session: 7, content: "1. Identify building blocks\n2. Choose operator preserving them\n3. Consider biases\n4. Test empirically", explanation: "Design: Identify building blocks, choose operator preserving them, consider biases, test empirically.", keyPoints: ["Identify building blocks", "Choose preserving operator", "Consider biases", "Test empirically"] },
  { number: 82, title: "Order-1 Example", session: 7, content: "P1: (1 2 3|4 5 6 7|8 9)\nP2: (4 5 2 1 8 7 6 9 3)\n\nCopy segment: _ _ _|4 5 6 7|_ _\nFill from P2: 8 9 3|4 5 6 7|2 1", explanation: "Order-1 detailed: Copy segment from P1, start after segment in P2, fill in order skipping existing values.", keyPoints: ["Copy segment from P1", "Fill from P2 in order", "Skip existing values"] },
  { number: 83, title: "PMX Example", session: 7, content: "Copy segment, create mapping:\n4↔1, 5↔8, 6↔7, 7↔6\n\nFill from P2, use mapping for conflicts", explanation: "PMX detailed: Copy segment, create mapping between segment elements, fill from P2 using mapping for conflicts.", keyPoints: ["Copy segment", "Create mapping", "Use mapping for conflicts"] },
  { number: 84, title: "Edge Table Construction", session: 7, content: "P1: A-B-C-D-E edges: AB,BC,CD,DE,EA\nP2: B-D-A-C-E edges: BD,DA,AC,CE,EB\n\nEdge table lists all neighbors from both", explanation: "Edge table: List all neighbors from both parents. Mark common edges (in both). Used to construct child preserving edges.", keyPoints: ["List all neighbors", "Mark common edges", "Guides child construction"] },
  { number: 85, title: "Edge Crossover Algorithm", session: 7, content: "1. Start with random city\n2. Remove from all lists\n3. Move to neighbor with shortest list\n4. Prefer common edges\n5. Repeat", explanation: "Edge algorithm: Start random, remove current from lists, move to neighbor with shortest remaining list, prefer common edges.", keyPoints: ["Start random", "Remove from lists", "Shortest remaining list", "Prefer common"] },
  { number: 86, title: "TSP Performance", session: 7, content: "Edge: Best (preserves what matters)\nPMX: Good\nOrder-1: Moderate\nCycle: Poor\n\nEdge preservation key for TSP!", explanation: "TSP performance: Edge best (preserves adjacencies = what matters). PMX good, Order-1 moderate, Cycle poor. Match operator to problem!", keyPoints: ["Edge best for TSP", "TSP depends on edges", "Match operator to problem"] },
  { number: 87, title: "Inver-Over", session: 7, content: "Hybrid crossover-mutation for TSP:\n1. Select random city\n2. Select c' (random or from another individual)\n3. Invert segment between them\n4. Repeat", explanation: "Inver-Over: Hybrid for TSP. Combines population info with 2-opt local search. Very effective but more complex.", keyPoints: ["Hybrid crossover-mutation", "Uses 2-opt moves", "Very effective for TSP"] },
  { number: 88, title: "Multi-Parent Crossover", session: 7, content: "Can extend to k parents:\n- Majority voting\n- Edge frequency\n- Consensus building\n\nUseful when converged", explanation: "Multi-parent: Use k parents via voting, edge frequency, consensus. Useful when population converged. Caution: bad if parents in different basins.", keyPoints: ["Can use k parents", "Voting, frequency, consensus", "Useful when converged", "Bad if different basins"] },
  { number: 89, title: "Session 7 Summary", session: 7, content: "Match crossover to problem\nOrder-1 for order, Edge for adjacency\nUnderstand biases\nTest empirically", explanation: "Summary: Match crossover to problem structure. Order-1 for order, Edge for adjacency. Understand biases. Always test.", keyPoints: ["Match to problem", "Order-1: order; Edge: adjacency", "Understand biases", "Test empirically"] },
  // Fill remaining session 7 slides (90-99) with mutation operators and related content
  { number: 90, title: "Permutation Mutation", session: 7, content: "Mutation operators for permutations:\n- Swap\n- Insert\n- Inversion\n- Scramble", explanation: "Permutation mutation operators that preserve validity: Swap, Insert, Inversion, Scramble. Each has different properties.", keyPoints: ["Swap, Insert, Inversion, Scramble", "All preserve permutation", "Different properties"] },
  { number: 91, title: "Swap Mutation", session: 7, content: "Swap: exchange values at two random positions\n\n[1,2,3,4,5] → [1,4,3,2,5]", explanation: "Swap mutation: Exchange two random positions. Simple, O(1), preserves permutation. Most common mutation.", keyPoints: ["Exchange two positions", "Simple O(1)", "Most common"] },
  { number: 92, title: "Insert Mutation", session: 7, content: "Insert: remove element, insert elsewhere\n\n[1,2,3,4,5] → [1,3,4,2,5]", explanation: "Insert mutation: Remove one element, insert at new position. Other elements shift. Preserves permutation.", keyPoints: ["Remove and insert elsewhere", "Elements shift", "Preserves permutation"] },
  { number: 93, title: "Inversion Mutation", session: 7, content: "Inversion: reverse a substring\n\n[1,2,3,4,5] → [1,4,3,2,5]", explanation: "Inversion mutation: Select substring, reverse it. Useful for TSP (2-opt move). Preserves permutation.", keyPoints: ["Reverse substring", "2-opt move for TSP", "Preserves permutation"] },
  { number: 94, title: "Scramble Mutation", session: 7, content: "Scramble: randomly reorder a substring\n\n[1,2,3,4,5] → [1,4,2,3,5]", explanation: "Scramble mutation: Select substring, randomly shuffle it. More disruptive than others. Preserves permutation.", keyPoints: ["Shuffle substring", "More disruptive", "Preserves permutation"] },
  { number: 95, title: "Mutation Comparison", session: 7, content: "Swap: minimal change, 2 positions\nInsert: moves one element\nInversion: reverses segment\nScramble: most disruptive", explanation: "Comparison: Swap minimal (2 positions), Insert moves one, Inversion reverses segment, Scramble most disruptive.", keyPoints: ["Swap: minimal", "Insert: moves one", "Inversion: reverses", "Scramble: most disruptive"] },
  { number: 96, title: "Mutation for TSP", session: 7, content: "For TSP:\n- Inversion preferred (2-opt)\n- Swap also effective\n- Match to local search moves", explanation: "For TSP: Inversion preferred (equivalent to 2-opt move). Swap also effective. Match mutation to known local search moves.", keyPoints: ["Inversion preferred for TSP", "Equivalent to 2-opt", "Match to local search"] },
  { number: 97, title: "Mutation Rate", session: 7, content: "Typical: 1/n (one swap expected per individual)\n\nHigher for diversity\nLower near convergence", explanation: "Typical mutation rate 1/n (one expected change). Higher for diversity maintenance, can lower near convergence.", keyPoints: ["Typical 1/n", "Higher for diversity", "Adjust as needed"] },
  { number: 98, title: "Combining Operators", session: 7, content: "Can use multiple mutation types:\n- Swap for fine-tuning\n- Inversion for larger changes\n- Scramble for diversity", explanation: "Can combine mutation types: Swap for fine-tuning, Inversion for larger changes, Scramble for diversity injection.", keyPoints: ["Combine types", "Different purposes", "Swap/Inversion/Scramble"] },
  { number: 99, title: "Session 7 Complete Summary", session: 7, content: "Permutation representation for ordering\nMatch crossover to building blocks\nMatch mutation to search needs\nAlways test empirically", explanation: "Complete summary: Permutation for ordering, match crossover to building blocks (order vs adjacency), match mutation to search needs, always test.", keyPoints: ["Permutation for ordering", "Match crossover to building blocks", "Match mutation to search", "Test empirically"] },
  
  // ==================== SESSION 8: Mutation & Debate (Slides 100-114) ====================
  { number: 100, title: "Session #8", session: 8, content: "Session #8: Mutation Operators & Debate Resolution", explanation: "Deep dive into mutation operators. Resolution of crossover vs mutation debate: crossover EXPLORES (large steps), mutation EXPLOITS (small steps, creates new).", keyPoints: ["Mutation operators in detail", "Debate resolution", "Crossover explores, mutation exploits"] },
  { number: 101, title: "Mutation Overview", session: 8, content: "Mutation: unary operator\nSmall random changes\nONLY source of new alleles\nMaintains diversity", explanation: "Mutation is unary operator making small random changes. ONLY source of new alleles in population. Critical for diversity maintenance.", keyPoints: ["Mutation is a unary operator that acts on a single individual rather than combining two parents", "It introduces small random changes that typically explore the local neighborhood of a solution", "Mutation is the ONLY source of new alleles—without it, information not in the initial population is forever lost", "By continuously introducing variation, mutation maintains diversity and prevents premature convergence"] },
  { number: 102, title: "Binary Mutation", session: 8, content: "Bit-flip: flip each bit with probability pm\n\nTypical pm = 1/l\n\n10110 → 10010", explanation: "Bit-flip: Flip each bit with probability pm. Typical pm = 1/l (one expected flip). Simple and effective.", keyPoints: ["Flip with probability pm", "pm = 1/l typical", "Simple effective"] },
  { number: 103, title: "Real-Valued Mutation", session: 8, content: "Gaussian: add N(0,σ)\nx' = x + N(0,σ)\n\nσ controls step size", explanation: "Gaussian mutation: x' = x + N(0,σ). Standard deviation σ controls step size. Can be self-adaptive (σ evolves too).", keyPoints: ["x' = x + N(0,σ)", "σ controls step size", "Can be self-adaptive"] },
  { number: 104, title: "Uniform Mutation", session: 8, content: "Uniform: replace with random value from range\n\nMore disruptive than Gaussian", explanation: "Uniform mutation: Replace with random value from allowed range. More disruptive than Gaussian—use for stuck populations.", keyPoints: ["Replace with random value", "More disruptive", "Use when stuck"] },
  { number: 105, title: "Mutation Strength", session: 8, content: "Too weak: slow progress\nToo strong: destroys solutions\n\nBalance needed, often problem-dependent", explanation: "Too weak mutation: slow progress. Too strong: destroys solutions. Balance needed—often problem and stage dependent.", keyPoints: ["Too weak: slow", "Too strong: destroys", "Balance needed"] },
  { number: 106, title: "Self-Adaptive Mutation", session: 8, content: "ES approach: encode σ in chromosome\nσ evolves alongside solution\n\nSuccessful σ values propagate", explanation: "Self-adaptive mutation (ES): Encode mutation strength σ in chromosome. σ evolves too—successful values propagate.", keyPoints: ["Encode σ in chromosome", "σ evolves too", "Successful values propagate"] },
  { number: 107, title: "1/5 Success Rule", session: 8, content: "Rechenberg's 1/5 rule:\n- If >1/5 mutations successful: increase σ\n- If <1/5 successful: decrease σ\n\nAdapt during run", explanation: "1/5 success rule: If >1/5 mutations improve, increase σ. If <1/5 improve, decrease σ. Simple adaptive strategy.", keyPoints: [">1/5 success: increase σ", "<1/5 success: decrease σ", "Simple adaptive"] },
  { number: 108, title: "Mutation vs Crossover Role", session: 8, content: "Historical debate:\nGA: crossover primary\nES/EP: mutation primary\n\nResolution: different roles!", explanation: "Historical debate: GA emphasized crossover, ES/EP emphasized mutation. Resolution: They have DIFFERENT roles, not competing.", keyPoints: ["GA: crossover", "ES/EP: mutation", "Different roles, not competing"] },
  { number: 109, title: "Crossover = Exploration", session: 8, content: "Crossover is EXPLORATIVE:\n- Large steps in search space\n- Combines existing solutions\n- Cannot create new alleles\n- Searches BETWEEN parents", explanation: "Crossover is EXPLORATIVE: Large steps, combines existing, cannot create new alleles, searches between parents in solution space.", keyPoints: ["Crossover is EXPLORATIVE, making large jumps by combining genetic material from two different individuals", "It can only recombine existing alleles in the population—it cannot introduce anything genuinely new", "The offspring represents a point somewhere in the region between the two parents in solution space", "This exploration role is complementary to mutation's exploitation role, not competing with it"] },
  { number: 110, title: "Mutation = Exploitation", session: 8, content: "Mutation is EXPLOITATIVE:\n- Small steps around current\n- Creates NEW genetic material\n- Fine-tunes solutions\n- Local search character", explanation: "Mutation is EXPLOITATIVE: Small steps, creates NEW material, fine-tunes, local search character around current solution.", keyPoints: ["Mutation is EXPLOITATIVE, making small perturbations to fine-tune solutions in a local neighborhood", "Crucially, mutation is the ONLY operator that can introduce genuinely new genetic material into the population", "Without mutation, any allele not present in the initial population can never be discovered by the GA", "This creates a local search character that complements crossover's global exploration"] },
  { number: 111, title: "Why Both Needed", session: 8, content: "Crossover only: cannot introduce new alleles\nMutation only: slow (small steps only)\n\nTogether: explore broadly + fine-tune locally", explanation: "Crossover only: cannot add new alleles, stagnates. Mutation only: works but slow. Together: broad exploration + local fine-tuning.", keyPoints: ["Crossover only: stagnates", "Mutation only: slow", "Together: explore + fine-tune"] },
  { number: 112, title: "One-Max Example", session: 8, content: "OneMax: count 1s in binary string\n\nWorst individual (all 0s) still has value:\n- Contains positions for 1s\n- Mutation can flip them\n- Without mutation: lost forever", explanation: "OneMax example: Even worst individual (all 0s) has value—positions exist to flip to 1. Without mutation chance, optimal alleles lost forever.", keyPoints: ["Worst still has potential", "Mutation can flip to 1", "Without mutation: lost forever"] },
  { number: 113, title: "Debate Resolution", session: 8, content: "Resolution:\n1. Both have different roles\n2. Crossover explores combinations\n3. Mutation creates new + fine-tunes\n4. Balance depends on problem", explanation: "Resolution: Different roles (explore vs exploit), crossover explores combinations, mutation creates and fine-tunes, balance problem-dependent.", keyPoints: ["Different roles", "Crossover: combinations", "Mutation: creates + fine-tunes", "Balance problem-dependent"] },
  { number: 114, title: "Session 8 Summary", session: 8, content: "Mutation operators: bit-flip, Gaussian, etc.\nCrossover EXPLORES, Mutation EXPLOITS\nBoth needed for effective search\nOnly mutation creates new alleles", explanation: "Summary: Various mutation operators, crossover explores (large), mutation exploits (small), both needed, only mutation creates new.", keyPoints: ["Various operators", "Crossover explores", "Mutation exploits", "Both needed"] },
  
  // ==================== SESSION 9: Multi-Parent & Population Models (Slides 115-131) ====================
  { number: 115, title: "Session #9", session: 9, content: "Session #9: Multi-Parent Recombination & Population Models", explanation: "Multi-parent recombination (more than 2 parents). Population models: generational vs steady-state. When multi-parent helps vs hurts.", keyPoints: ["Multi-parent recombination", "Population models", "When helps vs hurts"] },
  { number: 116, title: "Multi-Parent Motivation", session: 9, content: "Standard crossover: 2 parents\nMulti-parent: k > 2 parents\n\nExtract more information from population", explanation: "Standard crossover uses 2 parents. Multi-parent uses k>2 to extract more population information. Useful when population converged.", keyPoints: ["Standard: 2 parents", "Multi-parent: k > 2", "More information from population"] },
  { number: 117, title: "U-Scan Crossover", session: 9, content: "U-Scan: For each position, randomly select parent\nAll parents equally likely\n\nSimple extension of uniform crossover", explanation: "U-Scan: For each position, randomly select from any of k parents (equal probability). Simple extension of uniform crossover to k parents.", keyPoints: ["Each position from random parent", "Equal probability", "Extension of uniform"] },
  { number: 118, title: "Occurrence-Based Scan", session: 9, content: "OB-Scan: Choose most frequent value at each position\n\nConsensus building", explanation: "Occurrence-Based Scan: At each position, choose the value appearing most frequently among parents. Builds consensus.", keyPoints: ["Most frequent value", "Builds consensus"] },
  { number: 119, title: "Fitness-Based Scan", session: 9, content: "FB-Scan: Weight by fitness when choosing\n\nFitter parents contribute more", explanation: "Fitness-Based Scan: Choose values weighted by parent fitness. Fitter parents contribute more to offspring.", keyPoints: ["Weighted by fitness", "Fitter contribute more"] },
  { number: 120, title: "When Multi-Parent Helps", session: 9, content: "Helps when parents in SAME basin of attraction\nCombines partial solutions effectively\nExtracts consensus from converged population", explanation: "Multi-parent helps when parents in SAME basin—combines partial solutions, extracts consensus. Useful for converged populations.", keyPoints: ["Parents in SAME basin", "Combines partial solutions", "Extracts consensus"] },
  { number: 121, title: "When Multi-Parent Hurts", session: 9, content: "Hurts when parents in DIFFERENT basins\nChild ends up BETWEEN optima\nNot in any basin - poor fitness", explanation: "Multi-parent HURTS when parents in DIFFERENT basins. Child ends up between optima—not in any basin, poor fitness.", keyPoints: ["Parents in DIFFERENT basins", "Child between optima", "Poor fitness"], professorNote: "Multi-parent helps SAME basin, hurts DIFFERENT basins—child between optima." },
  { number: 122, title: "Population Models", session: 9, content: "Two main models:\n- Generational\n- Steady-State\n\nDiffer in replacement strategy", explanation: "Two population models: Generational (all replaced each generation) and Steady-State (few replaced at a time). Different dynamics.", keyPoints: ["Generational", "Steady-State", "Different replacement"] },
  { number: 123, title: "Generational Model", session: 9, content: "Generational: Create λ offspring, replace all μ parents\n\nComplete generation replacement\nClear generational structure", explanation: "Generational: Create λ offspring from μ parents, offspring completely replace parents. Clear generation boundaries.", keyPoints: ["Offspring replace all parents", "Complete replacement", "Clear generations"] },
  { number: 124, title: "Steady-State Model", session: 9, content: "Steady-State: Create 1-2 offspring, replace worst\n\nContinuous replacement\nNo clear generations", explanation: "Steady-State: Create 1-2 offspring per step, replace worst individual(s). Continuous flow, no clear generation boundaries.", keyPoints: ["Few offspring per step", "Replace worst", "Continuous, no clear generations"] },
  { number: 125, title: "Model Comparison", session: 9, content: "Generational: more diversity, slower convergence\nSteady-State: faster convergence, less diversity\n\nChoice depends on problem", explanation: "Generational: more diversity, slower convergence. Steady-State: faster convergence but less diversity. Choice problem-dependent.", keyPoints: ["Generational: more diversity", "Steady-State: faster convergence", "Problem-dependent"] },
  { number: 126, title: "Generation Gap", session: 9, content: "Generation Gap: fraction of population replaced\n\nG=1.0: Generational (all replaced)\nG=1/μ: Steady-State (one replaced)", explanation: "Generation Gap G: fraction replaced. G=1.0 is generational, G=1/μ is steady-state. Intermediate values possible.", keyPoints: ["G = fraction replaced", "G=1.0: generational", "G=1/μ: steady-state"] },
  { number: 127, title: "Elitism", session: 9, content: "Elitism: Always keep best individual(s)\n\nPrevents losing best-found solution\nCan combine with any model", explanation: "Elitism: Always copy best individual(s) to next generation. Prevents losing best solution. Combines with any model.", keyPoints: ["Keep best individuals", "Prevents losing best", "Combines with any model"] },
  { number: 128, title: "(μ+λ) Selection", session: 9, content: "(μ+λ): Select best μ from μ parents + λ offspring\n\nParents can survive\nElitist by nature", explanation: "(μ+λ) selection: Best μ from combined parents and offspring. Parents can survive. Naturally elitist.", keyPoints: ["Best μ from parents+offspring", "Parents can survive", "Naturally elitist"] },
  { number: 129, title: "(μ,λ) Selection", session: 9, content: "(μ,λ): Select best μ from λ offspring only\n\nParents always die\nλ > μ required", explanation: "(μ,λ) selection: Best μ from λ offspring only. Parents always die (λ > μ required). Forces exploration.", keyPoints: ["Best μ from offspring only", "Parents always die", "Requires λ > μ"] },
  { number: 130, title: "Model Choice Guidelines", session: 9, content: "Complex landscape: Generational (diversity)\nKnown good region: Steady-State (exploitation)\nNeed best guarantee: Add elitism", explanation: "Guidelines: Complex landscape → generational (diversity). Known good region → steady-state (exploit). Need best → add elitism.", keyPoints: ["Complex: generational", "Known region: steady-state", "Need best: elitism"] },
  { number: 131, title: "Session 9 Summary", session: 9, content: "Multi-parent: helps same basin, hurts different\nGenerational: diversity, slow\nSteady-State: fast, less diverse\nElitism: keeps best", explanation: "Summary: Multi-parent helps same basin/hurts different. Generational for diversity. Steady-state for speed. Elitism keeps best.", keyPoints: ["Multi-parent: same basin helps", "Generational: diversity", "Steady-state: speed", "Elitism: keeps best"] },
  
  // ==================== SESSION 10: Selection Mechanisms (Slides 132-155) ====================
  { number: 132, title: "Session #10", session: 10, content: "Session #10: Selection Mechanisms In-Depth", explanation: "Deep dive into selection. FPS problems exposed. Rank-based and Tournament selection as solutions.", keyPoints: ["Selection mechanisms", "FPS problems", "Tournament selection"] },
  { number: 133, title: "Selection Purpose", session: 10, content: "Selection identifies good solutions\nParent selection: who reproduces\nSurvivor selection: who continues\n\nDrives search toward optima", explanation: "Selection identifies good solutions. Parent selection chooses reproducers. Survivor selection chooses survivors. Drives search toward optima.", keyPoints: ["Identifies good solutions", "Parent vs Survivor", "Drives toward optima"] },
  { number: 134, title: "Selection Pressure", session: 10, content: "Selection pressure: degree of preference for better\n\nToo high: premature convergence\nToo low: random search", explanation: "Selection pressure: How strongly better individuals preferred. Too high → premature convergence. Too low → random search.", keyPoints: ["Preference for better", "Too high: premature", "Too low: random"], definitions: ["selectionPressure"] },
  { number: 135, title: "FPS Recap", session: 10, content: "FPS: P(i) = f(i)/Σf(j)\n\nE(n_i) = μ × f(i)/⟨f⟩", explanation: "FPS: Probability proportional to fitness. Expected copies = population size × (fitness / average fitness).", keyPoints: ["P(i) = f(i)/Σf(j)", "E(n_i) = μ × f(i)/⟨f⟩"], formulas: ["expectedCopiesFPS"] },
  { number: 136, title: "FPS Problem 1: Premature Convergence", session: 10, content: "Problem: One super-fit individual dominates\n\nTakes most selection slots\nDiversity rapidly lost\nStuck at local optimum", explanation: "FPS Problem 1: Super-fit individual dominates selection, takes most slots, diversity lost, stuck at local optimum.", keyPoints: ["Super-fit dominates", "Takes most slots", "Diversity lost", "Local optimum"] },
  { number: 137, title: "FPS Problem 2: Lost Selection Pressure", session: 10, content: "Problem: Late in run, fitness values similar\n\nAll get similar probability\nSelection becomes random\nCannot distinguish good from best", explanation: "FPS Problem 2: When fitness similar, probabilities similar, selection random, cannot distinguish good from best.", keyPoints: ["Similar fitness", "Random selection", "Cannot distinguish"] },
  { number: 138, title: "FPS Problem 3: Scaling Sensitivity", session: 10, content: "Problem: Adding constant changes behavior\n\nf: 10,20,30 → P: 1/6, 2/6, 3/6\nf+100: 110,120,130 → P: 0.31, 0.33, 0.36\n\nDramatically different!", explanation: "FPS Problem 3: Adding constant dramatically changes probabilities. Same ranking but very different selection. Uses EXACT values but we have CORRECT only.", keyPoints: ["Adding constant changes all", "Same ranking, different selection", "Uses exact, have correct"], professorNote: "FPS uses EXACT fitness but we only have CORRECT fitness (rankings)." },
  { number: 139, title: "Rank-Based Selection", session: 10, content: "Rank-Based: Use RANK not fitness value\n\nP(i) = (1/μ)[s - (s-1)(rank-1)/(μ-1)]\n\nImmune to scaling!", explanation: "Rank-based: Use RANK instead of fitness value. Immune to scaling/shifting. Parameter s controls pressure.", keyPoints: ["Use RANK not value", "Immune to scaling", "s controls pressure"], formulas: ["linearRanking"] },
  { number: 140, title: "Rank Selection Benefits", session: 10, content: "Rank solves:\n- Scaling sensitivity ✓\n- Maintains pressure late ✓\n\nBut: Requires sorting O(n log n)", explanation: "Rank-based solves scaling sensitivity and maintains pressure. But requires sorting O(n log n). Better than FPS.", keyPoints: ["Solves scaling", "Maintains pressure", "Requires sorting"] },
  { number: 141, title: "Tournament Selection", session: 10, content: "Tournament Selection:\n1. Pick k random individuals\n2. Select the best one\n3. Repeat as needed\n\nSimple and effective!", explanation: "Tournament: Pick k random, select best. Simple, effective, addresses all FPS problems.", keyPoints: ["Pick k random", "Select best", "Simple and effective"], definitions: ["tournamentSelection"] },
  { number: 142, title: "Tournament Size Effect", session: 10, content: "k=1: Random selection (no pressure)\nk=2: Binary tournament (mild pressure)\nk=N: Deterministic best (max pressure)", explanation: "Tournament size k controls pressure. k=1 random, k=2 mild, k=large strong, k=N always best.", keyPoints: ["k=1: random", "k=2: mild", "k=N: max pressure"] },
  { number: 143, title: "Tournament Advantages", session: 10, content: "Tournament advantages:\n- No sorting needed\n- Not scaling sensitive\n- Only needs comparisons (correct fitness)\n- Single parameter k\n- Parallelizable", explanation: "Tournament advantages: No sorting, not scaling sensitive, only needs comparisons, single parameter k, parallelizable. Addresses ALL FPS problems.", keyPoints: ["No sorting", "Not scaling sensitive", "Only comparisons", "Single parameter k", "Parallelizable"] },
  { number: 144, title: "Why Tournament Works", session: 10, content: "Only uses COMPARISONS:\n'Is A better than B?'\n\nDoes NOT use exact fitness values\nRobust to any monotonic transformation", explanation: "Tournament only uses comparisons (is A better than B?). Does NOT use exact values. Robust to monotonic transformations. Uses CORRECT fitness only.", keyPoints: ["Only comparisons", "Not exact values", "Robust to transformations"], professorNote: "Tournament uses only CORRECT fitness (comparisons), not EXACT values. This is why it's robust." },
  { number: 145, title: "Tournament Math", session: 10, content: "ŝ(f_i) = N × [(C(f_i)/N)^t - (C(f_{i-1})/N)^t]\n\nC(f_i) = cumulative count ≤ f_i\nt = tournament size", explanation: "Tournament math: Expected distribution after selection. Winner has highest fitness among t competitors.", keyPoints: ["Expected distribution formula", "Cumulative count", "Winner has highest"], formulas: ["tournamentExpected"] },
  { number: 146, title: "Tournament Verification", session: 10, content: "t=1: Random selection (verified)\nŝ(f_i) = N × [C(f_i)/N - C(f_{i-1})/N] = s(f_i)\n\nDistribution unchanged", explanation: "Verification: t=1 gives ŝ(f_i) = s(f_i), distribution unchanged = random selection. Formula correct.", keyPoints: ["t=1: random", "Distribution unchanged", "Formula verified"] },
  { number: 147, title: "Reproduction Rate", session: 10, content: "Reproduction Rate: R(f_i) = ŝ(f_i)/s(f_i)\n\nR > 1: fitness class grows\nR < 1: fitness class shrinks", explanation: "Reproduction rate R = after/before. R>1 means class grows, R<1 means shrinks.", keyPoints: ["R = after/before", "R>1: grows", "R<1: shrinks"], formulas: ["reproductionRate"] },
  { number: 148, title: "Selection Intensity", session: 10, content: "Selection Intensity: I = (f̄' - f̄)/σ_f\n\nMeasures pressure in standard deviations\nHigher I = faster convergence", explanation: "Selection intensity I = improvement in standard deviations. Higher I = faster convergence but more diversity loss.", keyPoints: ["I = improvement/σ", "Higher I = faster", "But more diversity loss"], formulas: ["selectionIntensity"] },
  { number: 149, title: "Diversity Loss", session: 10, content: "Loss of Diversity: P_d = proportion not selected\n\nFor tournament: P_d = (C(f_z)/N)^t\n\nLarger t = more loss", explanation: "Diversity loss: proportion never selected. For tournament, increases with t.", keyPoints: ["Proportion not selected", "Increases with t"], formulas: ["lossOfDiversity"] },
  { number: 150, title: "Truncation Selection", session: 10, content: "Truncation: Select top τ fraction only\n\nVery strong pressure\nUsed in some ES variants", explanation: "Truncation: Only top τ% selected. Very strong pressure. Used in some ES variants.", keyPoints: ["Only top τ%", "Very strong", "Used in ES"] },
  { number: 151, title: "Boltzmann Selection", session: 10, content: "Boltzmann: P(i) ∝ exp(f(i)/T)\n\nTemperature T controls pressure\nHigh T = low pressure, Low T = high pressure", explanation: "Boltzmann selection: Probability proportional to exp(f/T). Temperature controls pressure. Similar to simulated annealing.", keyPoints: ["P ∝ exp(f/T)", "T controls pressure", "Like simulated annealing"] },
  { number: 152, title: "Comparison Summary", session: 10, content: "FPS: Simple but problematic\nRank: Better but needs sorting\nTournament: Best all-around\nTruncation: Very strong pressure", explanation: "Summary: FPS simple but problematic. Rank better but sorting. Tournament best all-around. Truncation very strong.", keyPoints: ["FPS: problematic", "Rank: better, sorting", "Tournament: best", "Truncation: very strong"] },
  { number: 153, title: "Selection Guidelines", session: 10, content: "Default: Tournament (k=2 to 7)\nNeed diversity: Lower k\nNeed speed: Higher k\nAlways: Keep some weak individuals", explanation: "Guidelines: Default tournament k=2-7. Lower k for diversity, higher for speed. Always keep some weak individuals.", keyPoints: ["Default: tournament", "Lower k: diversity", "Higher k: speed", "Keep weak individuals"] },
  { number: 154, title: "Session 10 Key Insight", session: 10, content: "KEY INSIGHT:\nWe know CORRECT fitness (rankings)\nWe don't know EXACT fitness (values)\n\nTournament uses only correct → ROBUST", explanation: "KEY INSIGHT: We have CORRECT (rankings) not EXACT (values). Tournament uses only correct fitness → robust to scaling.", keyPoints: ["Have CORRECT not EXACT", "Tournament uses correct", "Therefore robust"], professorNote: "This is THE key insight: we have correct fitness (rankings), not exact (values). Tournament selection is robust because it only uses comparisons." },
  { number: 155, title: "Session 10 Summary", session: 10, content: "FPS has 3 major problems\nTournament addresses all\nOnly needs comparisons\nSingle parameter k controls pressure", explanation: "Summary: FPS has 3 problems (premature, lost pressure, scaling). Tournament addresses all, needs only comparisons, k controls pressure.", keyPoints: ["FPS: 3 problems", "Tournament: all solved", "Only comparisons", "k controls pressure"] },
  
  // ==================== SESSION 11: Schema Theory (Slides 156-177) ====================
  { number: 156, title: "Session #11", session: 11, content: "Session #11: Schema Theorem & Building Block Hypothesis", explanation: "Holland's Schema Theorem. Schema definition. Order and defining length. Building Block Hypothesis. Limitations and criticisms.", keyPoints: ["Schema Theorem", "Building blocks", "Limitations"] },
  { number: 157, title: "Schema Definition", session: 11, content: "Schema: template over {0, 1, #}\n# = wildcard/don't care\n\n1##0# matches: 10000, 10001, 10100, 10101, 11000, 11001, 11100, 11101", explanation: "Schema: Template over {0,1,#} where # is wildcard. 1##0# matches all strings with 1 in position 1 and 0 in position 4.", keyPoints: ["Template over {0,1,#}", "# = wildcard", "Matches multiple chromosomes"], definitions: ["schema"] },
  { number: 158, title: "Schema as Hyperplane", session: 11, content: "Schema represents hyperplane in solution space\n\nk wildcards → 2^k matching chromosomes\n\nEach chromosome matches 2^l schemata", explanation: "Schema = hyperplane. k wildcards matches 2^k chromosomes. Each chromosome matches 2^l schemata (each position: match defined or use wildcard).", keyPoints: ["Hyperplane in space", "k wildcards → 2^k matches", "Chromosome matches 2^l schemata"] },
  { number: 159, title: "Order of Schema", session: 11, content: "Order o(H): number of defined (non-#) positions\n\no(1##0#) = 2\no(#####) = 0\no(10101) = 5", explanation: "Order o(H) = number of defined positions (non-wildcards). Affects mutation survival: (1-pm)^o(H).", keyPoints: ["o(H) = defined positions", "Low order survives mutation better"], definitions: ["order"], formulas: ["schemaDisruptionMutation"] },
  { number: 160, title: "Defining Length", session: 11, content: "Defining length δ(H): distance between first and last defined positions\n\nδ(1###0) = 4\nδ(##1#0##) = 2", explanation: "Defining length δ(H) = distance from first to last defined position. Affects crossover survival: 1 - pc×δ(H)/(l-1).", keyPoints: ["δ(H) = first to last defined", "Short survives crossover better"], definitions: ["definingLength"], formulas: ["schemaDisruptionXover"] },
  { number: 161, title: "Schema Fitness", session: 11, content: "Schema fitness f(H):\nAverage fitness of all individuals matching H\n\nAbove average → grows\nBelow average → shrinks", explanation: "Schema fitness f(H) = average fitness of matching individuals. Above average schemata grow, below average shrink.", keyPoints: ["Average of matching individuals", "Above average grows", "Below average shrinks"] },
  { number: 162, title: "Implicit Parallelism", session: 11, content: "N chromosomes process O(N³) schemata\n\nEach chromosome samples many schemata\nPopulation provides massive parallelism", explanation: "Implicit parallelism: N chromosomes effectively sample O(N³) schemata. Population provides parallel schema evaluation.", keyPoints: ["N chromosomes → O(N³) schemata", "Massive parallelism"], definitions: ["implicitParallelism"] },
  { number: 163, title: "Schema Theorem", session: 11, content: "m(H,t+1) ≥ m(H,t) × [f(H)/⟨f⟩] × [1 - pc×δ(H)/(l-1)] × (1-pm)^o(H)\n\nSelection × Crossover survival × Mutation survival", explanation: "Schema Theorem: Lower bound on schema proportion next generation. Three factors: selection (fitness ratio), crossover survival (defining length), mutation survival (order).", keyPoints: ["The Schema Theorem provides a lower bound for how schema proportions change from one generation to the next", "Selection factor (f(H)/⟨f⟩) rewards above-average schemata and penalizes below-average ones", "Crossover survival (1 - pc×δ(H)/(l-1)) favors short defining lengths that are unlikely to be disrupted by cuts", "Mutation survival ((1-pm)^o(H)) favors low-order schemata with fewer defined bits that could be flipped"], formulas: ["schemaTheorem"], definitions: ["schema"] },
  { number: 164, title: "Selection Factor", session: 11, content: "f(H)/⟨f⟩: Selection effect\n\nf(H) > ⟨f⟩ → factor > 1 → GROWS\nf(H) < ⟨f⟩ → factor < 1 → SHRINKS", explanation: "Selection factor f(H)/⟨f⟩: Above-average schemata have factor > 1, grow. Below-average have factor < 1, shrink.", keyPoints: ["Above average: grows", "Below average: shrinks"] },
  { number: 165, title: "Crossover Factor", session: 11, content: "1 - pc × δ(H)/(l-1): Crossover survival\n\nShort defining length → high survival\nLong defining length → low survival", explanation: "Crossover survival = 1 - pc×δ(H)/(l-1). Short defining length survives better. Cut must fall outside defining length.", keyPoints: ["Short δ: high survival", "Long δ: low survival"], formulas: ["schemaDisruptionXover"] },
  { number: 166, title: "Mutation Factor", session: 11, content: "(1-pm)^o(H): Mutation survival\n\nLow order → high survival\nHigh order → low survival\n\n≈ 1 - o(H)×pm for small pm", explanation: "Mutation survival = (1-pm)^o(H). Low order survives better. All o(H) defined bits must not be flipped.", keyPoints: ["Low order: high survival", "High order: low survival"], formulas: ["schemaDisruptionMutation"] },
  { number: 167, title: "Why Inequality", session: 11, content: "Theorem gives ≥ (lower bound), not =\n\nIgnores CONSTRUCTIVE effects:\n- Crossover creating schema members\n- Mutation creating schema members", explanation: "Inequality (≥) because ignores constructive effects. Crossover and mutation can create schema members, not just destroy.", keyPoints: ["Lower bound, not exact", "Ignores constructive effects"] },
  { number: 168, title: "Building Block Hypothesis", session: 11, content: "Building Block Hypothesis:\nShort, low-order, above-average schemata are building blocks\n\nGA works by discovering and combining them", explanation: "Building Block Hypothesis: GA discovers short, low-order, above-average schemata (building blocks) and combines them into solutions.", keyPoints: ["Building blocks are schemata that are short (small δ), low-order (few defined bits), and above-average fitness", "The GA's implicit parallelism samples many building blocks simultaneously across the population", "Selection amplifies good building blocks while crossover combines them into increasingly fit solutions", "The BBH guides design: keep related genes close together and choose representations where building blocks naturally exist"], definitions: ["buildingBlock"] },
  { number: 169, title: "BBH Implications", session: 11, content: "Design implications:\n- Keep related genes close (short δ)\n- Use representations where BBs exist\n- Crossover should preserve BBs", explanation: "BBH implies: Keep related genes close (short defining length), use representations with good building blocks, crossover should preserve them.", keyPoints: ["Related genes close", "Good representations", "Preserve in crossover"] },
  { number: 170, title: "Schema Theorem Limitations", session: 11, content: "Limitations:\n1. Only lower bound\n2. Says nothing about t+2\n3. Assumes FPS (rarely used)\n4. Royal Road counter-examples", explanation: "Limitations: Only lower bound, says nothing about t+2, assumes FPS, Royal Road counter-examples show BBH doesn't always hold.", keyPoints: ["Only lower bound", "Nothing about t+2", "Assumes FPS", "Counter-examples exist"] },
  { number: 171, title: "Criticism 1: Lower Bound", session: 11, content: "Only gives lower bound\nActual growth can be much higher\nNot predictive of actual behavior", explanation: "Criticism: Only lower bound, can be very loose. Actual behavior may differ significantly. Not useful for prediction.", keyPoints: ["Lower bound only", "Can be loose", "Not predictive"] },
  { number: 172, title: "Criticism 2: One Step", session: 11, content: "Only predicts t to t+1\nSays nothing about long-term\nCannot compound over generations", explanation: "Criticism: Only one step prediction (t to t+1). Cannot compound over generations. Long-term behavior unknown.", keyPoints: ["Only one step", "No long-term", "Cannot compound"] },
  { number: 173, title: "Criticism 3: Assumes FPS", session: 11, content: "Assumes Fitness-Proportionate Selection\nFPS rarely used in practice\nTournament selection has different dynamics", explanation: "Criticism: Assumes FPS which is rarely used. Tournament selection has different dynamics not captured by theorem.", keyPoints: ["Assumes FPS", "FPS rarely used", "Different dynamics with tournament"] },
  { number: 174, title: "Despite Criticisms", session: 11, content: "Despite criticisms, Schema Theorem:\n- Provides conceptual framework\n- Guides representation design\n- Explains why short fit schemata matter", explanation: "Despite limitations, Schema Theorem valuable as conceptual framework, guides design, explains importance of short fit schemata.", keyPoints: ["Conceptual framework", "Guides design", "Explains short fit schemata"] },
  { number: 175, title: "Modern View", session: 11, content: "Modern view:\n- Useful conceptual tool\n- Not predictive theory\n- BBH has significant exceptions\n- Empirical testing essential", explanation: "Modern view: Useful conceptually but not predictive. BBH has exceptions. Empirical testing always essential.", keyPoints: ["Useful conceptually", "Not predictive", "BBH has exceptions", "Test empirically"] },
  { number: 176, title: "Practical Guidance", session: 11, content: "Practical guidance from Schema Theory:\n1. Keep related genes together\n2. Use appropriate crossover\n3. Don't rely on theory alone", explanation: "Practical guidance: Keep related genes together, use appropriate crossover, don't rely on theory alone—test.", keyPoints: ["Related genes together", "Appropriate crossover", "Test, don't just theorize"] },
  { number: 177, title: "Session 11 Summary", session: 11, content: "Schema: template matching multiple chromosomes\nTheorem: short, low-order, fit schemata grow\nBBH: GA combines building blocks\nLimitations: Lower bound, FPS, counter-examples", explanation: "Summary: Schemata are templates, theorem says short/low-order/fit grow, BBH says GA combines BBs, but many limitations exist.", keyPoints: ["Schemata = templates", "Short, fit grow", "BBH: combine BBs", "Many limitations"] },
  
  // ==================== SESSION 12: Royal Road (Slides 178-201) ====================
  { number: 178, title: "Session #12", session: 12, content: "Session #12: Royal Road Functions & GA Performance", explanation: "Royal Road functions designed to be GA-easy. Surprising results: removing intermediate levels helped. Analysis of why.", keyPoints: ["Royal Road functions", "Unexpected results", "Why intermediate hurts"] },
  { number: 179, title: "Royal Road Motivation", session: 12, content: "Royal Road: Design functions GA should excel at\n\nBased on Building Block Hypothesis:\n- Clear building blocks\n- Obvious combination path", explanation: "Royal Road functions designed to test BBH. Clear building blocks with obvious combination path. GA should excel.", keyPoints: ["Designed for GA", "Based on BBH", "Clear building blocks"] },
  { number: 180, title: "R1 Function", session: 12, content: "R1: Sum of order-8 schemata\n\n8 building blocks of 8 bits each\nFitness = sum of complete blocks\nMax = 64 (all 8 blocks)", explanation: "R1 function: 64-bit string, eight 8-bit blocks. Fitness = number of complete blocks × 8. Max = 64.", keyPoints: ["Eight 8-bit blocks", "Complete blocks score", "Max = 64"] },
  { number: 181, title: "R2 Function", session: 12, content: "R2: Hierarchical building blocks\n\nLevel 1: 8-bit blocks\nLevel 2: pairs of level 1\nLevel 3: pairs of level 2\n...", explanation: "R2: Hierarchical BBs. Level 1 = 8-bit blocks, Level 2 = pairs of level 1, etc. Intermediate stepping stones.", keyPoints: ["Hierarchical structure", "Multiple levels", "Stepping stones"] },
  { number: 182, title: "Expected Results", session: 12, content: "Expected:\nR1: GA should work well\nR2: GA should work even better (guided by intermediates)", explanation: "Expected: R1 should work well (clear BBs). R2 even better because intermediate levels guide search.", keyPoints: ["R1: should work", "R2: even better", "Intermediates guide"] },
  { number: 183, title: "Actual Results", session: 12, content: "Actual Results:\nR1: 61,334 evaluations (average)\nR2: Much worse!\n\nCounter to expectations!", explanation: "Actual: R1 took 61,334 evaluations. R2 was WORSE! Counter to BBH predictions.", keyPoints: ["R1: 61,334 evals", "R2: worse!", "Counter to expectations"] },
  { number: 184, title: "The Surprise", session: 12, content: "Even more surprising:\nRemoving intermediate levels IMPROVED performance!\n\nR1 (no intermediates): 61,334\nWith some intermediates: 427,tried (worse)", explanation: "Bigger surprise: Removing intermediate levels IMPROVED performance! Intermediate stepping stones actually hurt.", keyPoints: ["Removing intermediates helped", "Stepping stones hurt!"], professorNote: "This was shocking: removing intermediate levels helped GA performance." },
  { number: 185, title: "Why Intermediates Hurt", session: 12, content: "Why intermediates hurt:\n1. Destroy GA's parallelism\n2. Dictate solution path\n3. Create hitchhiking", explanation: "Why intermediates hurt: Destroy GA's parallelism (GA explores many paths), dictate single solution path, create hitchhiking.", keyPoints: ["Destroy parallelism", "Dictate path", "Create hitchhiking"] },
  { number: 186, title: "GA Parallelism", session: 12, content: "GA is parallel search:\n- Explores many regions\n- Multiple solutions develop\n- Building blocks combine flexibly", explanation: "GA is fundamentally parallel: explores many regions simultaneously, multiple solutions develop, flexible combination.", keyPoints: ["Explores many regions", "Multiple solutions", "Flexible combination"] },
  { number: 187, title: "Intermediates Destroy Parallelism", session: 12, content: "Intermediates destroy parallelism:\n- Force single path\n- Must complete level before next\n- Sequential, not parallel", explanation: "Intermediates force single sequential path. Must complete each level before progressing. Destroys parallel nature.", keyPoints: ["Force single path", "Sequential", "Destroys parallelism"] },
  { number: 188, title: "Hitchhiking Problem", session: 12, content: "Hitchhiking: bad alleles spread with good blocks\n\nGood block attracts selection\nBad nearby alleles come along\nHard to separate later", explanation: "Hitchhiking: Bad alleles spread along with good blocks. Selection for good block brings bad nearby alleles. Hard to separate.", keyPoints: ["Bad alleles spread", "Come with good blocks", "Hard to separate"] },
  { number: 189, title: "Hill Climbing Comparison", session: 12, content: "Shocking comparison:\nRandom Mutation Hill Climbing: 6,179 evaluations\nGA: 61,334 evaluations\n\nSimple HC beat GA by 10x!", explanation: "Random Mutation Hill Climbing solved R1 in 6,179 evaluations vs GA's 61,334. Simple HC beat GA by 10x!", keyPoints: ["HC: 6,179", "GA: 61,334", "HC 10x better!"] },
  { number: 190, title: "Lessons Learned", session: 12, content: "Lessons:\n1. BBH doesn't always hold\n2. More structure can hurt\n3. Theory doesn't predict well\n4. Test empirically!", explanation: "Lessons: BBH doesn't always hold, more structure can hurt, theory not predictive, always test empirically.", keyPoints: ["BBH not universal", "Structure can hurt", "Theory not predictive", "Test empirically"] },
  { number: 191, title: "Tanese Functions", session: 12, content: "Tanese functions: Another GA-designed test\n\nAlso showed GA worse than expected\nEpistasis (gene interactions) important", explanation: "Tanese functions: Another test showing GA worse than expected. Epistasis (gene interactions) affects difficulty more than structure.", keyPoints: ["GA worse than expected", "Epistasis important"] },
  { number: 192, title: "When GA Works", session: 12, content: "GA works well when:\n- Building blocks actually exist\n- BBs can combine independently\n- No deception\n- No hitchhiking", explanation: "GA works when: Building blocks exist, can combine independently, no deception, no hitchhiking problems.", keyPoints: ["BBs exist", "Independent combination", "No deception", "No hitchhiking"] },
  { number: 193, title: "When GA Struggles", session: 12, content: "GA struggles when:\n- Deceptive (BBs mislead)\n- Epistasis (complex interactions)\n- Imposed structure (sequential path)", explanation: "GA struggles: Deceptive problems, high epistasis (interactions), imposed sequential structure.", keyPoints: ["Deceptive problems", "High epistasis", "Sequential structure"] },
  { number: 194, title: "Domain Knowledge", session: 12, content: "Counterintuitive insight:\n'Whatever more domain knowledge involves, GA performance falls'\n\nToo much structure hurts parallel search", explanation: "Counterintuitive: More domain knowledge can hurt GA. Imposing structure destroys parallel search advantage.", keyPoints: ["Domain knowledge can hurt", "Structure destroys parallelism"], professorNote: "Whatever more domain knowledge involves with system, performance of GA will fall down." },
  { number: 195, title: "GA as Parallel Solver", session: 12, content: "GA is fundamentally parallel problem solver\n\nImposing sequential structure:\n- Negates this advantage\n- Forces single path\n- May be worse than simple methods", explanation: "GA is fundamentally parallel. Imposing sequence negates advantage, forces single path, can be worse than simple methods.", keyPoints: ["GA is parallel", "Sequence negates advantage", "Can be worse than simple"] },
  { number: 196, title: "Practical Implications", session: 12, content: "Practical implications:\n1. Don't over-constrain\n2. Let GA explore freely\n3. Avoid imposed sequences\n4. Compare with simple baselines", explanation: "Practical: Don't over-constrain, let GA explore freely, avoid imposed sequences, always compare with simple baselines.", keyPoints: ["Don't over-constrain", "Let explore freely", "Avoid sequences", "Compare baselines"] },
  { number: 197, title: "R1 vs R2 Analysis", session: 12, content: "R1: Flat structure, GA explores freely\nR2: Hierarchical, forced sequence\n\nFlat structure allows parallel exploration", explanation: "R1 flat allows parallel exploration. R2 hierarchical forces sequence. Flat better for GA.", keyPoints: ["R1 flat: free exploration", "R2 hierarchical: forced sequence", "Flat better for GA"] },
  { number: 198, title: "Comparison Methods", session: 12, content: "Always compare GA with:\n1. Random search\n2. Hill climbing\n3. Problem-specific heuristics\n\nGA not always best!", explanation: "Always compare GA with random search, hill climbing, heuristics. GA is not always best choice.", keyPoints: ["Compare with random", "Compare with HC", "Compare with heuristics", "GA not always best"] },
  { number: 199, title: "Choosing GA", session: 12, content: "Choose GA when:\n- Large search space\n- Multiple optima desired\n- Little domain knowledge\n- Need creative solutions", explanation: "Choose GA for: Large spaces, multiple optima, little domain knowledge, creative solutions needed.", keyPoints: ["Large space", "Multiple optima", "Little domain knowledge", "Creative solutions"] },
  { number: 200, title: "Session 12 Insights", session: 12, content: "Key insights:\n1. Theory has limits\n2. Structure can hurt\n3. GA is parallel solver\n4. Simple methods can win", explanation: "Key insights: Theory has limits, structure can hurt, GA is parallel, simple methods can win.", keyPoints: ["Theory limited", "Structure can hurt", "GA is parallel", "Simple can win"] },
  { number: 201, title: "Session 12 Summary", session: 12, content: "Royal Road showed BBH limitations\nIntermediate levels hurt performance\nGA is fundamentally parallel\nAlways test, don't assume", explanation: "Summary: Royal Road revealed BBH limitations. Intermediates hurt. GA is parallel. Always test empirically.", keyPoints: ["BBH limitations", "Intermediates hurt", "GA parallel", "Always test"] },
  
  // ==================== SESSION 13: GA Difficulty (Slides 202-216) ====================
  { number: 202, title: "Session #13", session: 13, content: "Session #13: GA Difficulty & Deception", explanation: "Four reasons for poor GA performance. Deception concept. Fitness-distance correlation.", keyPoints: ["Four failure reasons", "Deception", "Fitness-distance correlation"] },
  { number: 203, title: "Four Reasons for Failure", session: 13, content: "Four reasons GA fails:\n1. Representation doesn't match\n2. Fitness function misleads\n3. Operators don't fit\n4. Parameters wrong", explanation: "Four failure reasons: (1) Representation mismatch, (2) Misleading fitness, (3) Operator mismatch, (4) Wrong parameters.", keyPoints: ["Representation mismatch", "Misleading fitness", "Operator mismatch", "Wrong parameters"] },
  { number: 204, title: "Reason 1: Representation", session: 13, content: "Representation mismatch:\n- Doesn't capture problem structure\n- Building blocks don't exist\n- Neighbors not meaningful", explanation: "Representation mismatch: Doesn't capture structure, building blocks don't exist in encoding, neighbors not meaningful.", keyPoints: ["Doesn't capture structure", "No building blocks", "Poor neighbors"] },
  { number: 205, title: "Reason 2: Fitness", session: 13, content: "Misleading fitness:\n- Doesn't reflect true objective\n- Deceptive landscape\n- Local optima mislead", explanation: "Misleading fitness: Doesn't reflect true objective, deceptive landscape, local optima mislead toward wrong solutions.", keyPoints: ["Not true objective", "Deceptive", "Local optima mislead"] },
  { number: 206, title: "Reason 3: Operators", session: 13, content: "Operator mismatch:\n- Wrong for representation\n- Destroy building blocks\n- Poor neighborhood structure", explanation: "Operator mismatch: Wrong for representation, destroy building blocks, create poor neighborhood structure.", keyPoints: ["Wrong for representation", "Destroy building blocks", "Poor neighborhoods"] },
  { number: 207, title: "Reason 4: Parameters", session: 13, content: "Wrong parameters:\n- Population too small\n- Mutation rate wrong\n- Selection pressure inappropriate", explanation: "Wrong parameters: Population too small, mutation rate wrong (too high/low), selection pressure inappropriate.", keyPoints: ["Population too small", "Wrong mutation rate", "Bad selection pressure"] },
  { number: 208, title: "Deception Definition", session: 13, content: "Deception: Low-order schemata lead AWAY from optimum\n\nBuilding blocks point wrong direction\nGA misled by good-looking partial solutions", explanation: "Deception: Lower-order (smaller) schemata have optima leading away from global optimum. GA misled by partial solutions.", keyPoints: ["Low-order leads away", "BBs point wrong", "Partial solutions mislead"], definitions: ["deception"] },
  { number: 209, title: "Deceptive Example", session: 13, content: "Example:\nOptimum: 111\nBut 0** has higher average than 1**\nGA driven toward wrong region!", explanation: "Example: Global optimum is 111, but schema 0** has higher average fitness than 1**. GA driven toward wrong region.", keyPoints: ["Optimum is 111", "But 0** looks better", "GA misled"] },
  { number: 210, title: "Full Deception", session: 13, content: "Fully deceptive: ALL lower-order schemata mislead\n\nOptimum maximally isolated\nGA has no guidance toward it", explanation: "Fully deceptive: All lower-order schemata lead away from optimum. Optimum completely isolated. No guidance.", keyPoints: ["All lower-order mislead", "Optimum isolated", "No guidance"] },
  { number: 211, title: "Partial Deception", session: 13, content: "Partially deceptive: Some schemata mislead\n\nSome correct guidance exists\nGA may find with effort", explanation: "Partially deceptive: Some schemata mislead, some correct. GA may find optimum with effort.", keyPoints: ["Some mislead, some correct", "Some guidance", "May find with effort"] },
  { number: 212, title: "Deception Analysis Limits", session: 13, content: "Deception analysis has limits:\n- Doesn't fully predict difficulty\n- Other factors matter\n- Empirical testing needed", explanation: "Deception analysis limited: Doesn't fully predict difficulty, other factors matter, empirical testing essential.", keyPoints: ["Doesn't fully predict", "Other factors", "Test empirically"] },
  { number: 213, title: "Fitness-Distance Correlation", session: 13, content: "Fitness-Distance Correlation (FDC):\nCorrelation between fitness and distance to optimum\n\nFDC > 0: Easier (fitness guides)\nFDC < 0: Harder (fitness misleads)", explanation: "FDC: Correlation of fitness with distance to optimum. Positive = fitness guides toward optimum. Negative = misleading.", keyPoints: ["Correlation fitness-distance", "Positive: guides", "Negative: misleads"] },
  { number: 214, title: "FDC as Difficulty Measure", session: 13, content: "FDC as difficulty predictor:\n- Easy: FDC ≈ -1 (high fitness near optimum)\n- Hard: FDC ≈ 0 (no correlation)\n- Deceptive: FDC ≈ +1 (high fitness far from optimum)", explanation: "FDC predicts difficulty: FDC≈-1 easy (fitness increases near optimum), FDC≈0 hard (no correlation), FDC≈+1 deceptive.", keyPoints: ["-1: easy", "0: hard", "+1: deceptive"] },
  { number: 215, title: "Practical Difficulty Assessment", session: 13, content: "Practical assessment:\n1. Check representation fit\n2. Look for deception\n3. Calculate FDC if possible\n4. Test with simple methods", explanation: "Practical: Check representation, look for deception, calculate FDC if possible, test with simple methods as baseline.", keyPoints: ["Check representation", "Look for deception", "Calculate FDC", "Test with simple methods"] },
  { number: 216, title: "Session 13 Summary", session: 13, content: "Four failure reasons\nDeception misleads GA\nFDC measures difficulty\nAlways diagnose before assuming GA works", explanation: "Summary: Four reasons for failure, deception misleads, FDC measures difficulty, always diagnose problems.", keyPoints: ["Four failure reasons", "Deception misleads", "FDC measures", "Diagnose problems"] },
  
  // ==================== SESSION 14: Selection Analysis (Slides 217-248) ====================
  { number: 217, title: "Session #14", session: 14, content: "Session #14: Selection Analysis & Mathematical Foundations", explanation: "Mathematical analysis of selection. Expected copies formula. Tournament selection theorem. Reproduction rate, selection intensity, diversity loss.", keyPoints: ["Mathematical analysis", "Tournament theorem", "Selection metrics"] },
  { number: 218, title: "Selection Analysis Goals", session: 14, content: "Goals:\n- Understand selection mathematically\n- Compare methods precisely\n- Predict behavior\n- Guide parameter choices", explanation: "Goals: Understand selection mathematically, compare methods precisely, predict behavior, guide parameter choices.", keyPoints: ["Understand mathematically", "Compare precisely", "Predict", "Guide choices"] },
  { number: 219, title: "Fitness Distribution", session: 14, content: "Fitness distribution s(f):\nNumber of individuals with fitness f\n\nBefore selection: s(f)\nAfter selection: ŝ(f)", explanation: "Fitness distribution s(f) = count with fitness f. Before selection: s(f). After selection: ŝ(f).", keyPoints: ["s(f) = count with fitness f", "Before: s(f)", "After: ŝ(f)"] },
  { number: 220, title: "Cumulative Distribution", session: 14, content: "Cumulative distribution C(f):\nNumber with fitness ≤ f\n\nC(f) = Σ_{g≤f} s(g)", explanation: "Cumulative C(f) = count with fitness ≤ f. Useful for tournament selection analysis.", keyPoints: ["C(f) = count ≤ f", "Useful for tournament"] },
  { number: 221, title: "FPS Expected Derivation", session: 14, content: "FPS derivation:\nP(i) = f(i)/Σf(j) = f(i)/(μ⟨f⟩)\n\nE(n_i) = μ × P(i) = f(i)/⟨f⟩", explanation: "FPS derivation: Selection probability = f(i)/total = f(i)/(μ⟨f⟩). Expected copies = μ × probability.", keyPoints: ["P(i) = f(i)/total", "E(n_i) = μ × P(i)"], formulas: ["expectedCopiesFPS"] },
  { number: 222, title: "Variance of Selection", session: 14, content: "Variance: Var(n_i) = μ × p_i × (1-p_i)\n\nBinomial distribution\nHigh variance in roulette wheel", explanation: "Variance follows binomial: Var = μ × p × (1-p). Roulette wheel has high variance. SUS reduces it.", keyPoints: ["Binomial variance", "High in roulette", "SUS reduces"], formulas: ["varianceTheorem"] },
  { number: 223, title: "Tournament Analysis", session: 14, content: "Tournament: Winner has highest among k random\n\nP(winner ≤ f) = [C(f)/N]^k\n\nDistribution shifts toward high fitness", explanation: "Tournament: All k must have fitness ≤ f for winner ≤ f. Probability = [C(f)/N]^k. Distribution shifts toward high fitness.", keyPoints: ["All k must be ≤ f", "P = [C(f)/N]^k", "Shifts toward high"] },
  { number: 224, title: "Tournament Expected Formula", session: 14, content: "ŝ(f) = N × [(C(f)/N)^t - (C(f-1)/N)^t]\n\nDifference of cumulative powers", explanation: "Expected after tournament: ŝ(f) = N × [(C(f)/N)^t - (C(f-1)/N)^t]. Difference of cumulative distribution powers.", keyPoints: ["Difference of powers", "Cumulative based"], formulas: ["tournamentExpected"] },
  { number: 225, title: "Tournament Size Effect", session: 14, content: "As t increases:\n- [C(f)/N]^t decreases faster for low f\n- High fitness individuals dominate more\n- Selection pressure increases", explanation: "Larger tournament: Low fitness shrinks faster, high fitness dominates, pressure increases.", keyPoints: ["Low fitness shrinks faster", "High dominates", "Pressure increases"] },
  { number: 226, title: "t=1 Verification", session: 14, content: "t=1: ŝ(f) = N × [C(f)/N - C(f-1)/N]\n= C(f) - C(f-1) = s(f)\n\nRandom selection - correct!", explanation: "t=1 gives ŝ(f) = s(f), distribution unchanged = random selection. Verifies formula.", keyPoints: ["t=1: unchanged", "Random selection", "Formula verified"] },
  { number: 227, title: "Concatenation Theorem", session: 14, content: "Concatenation: Ω_t2(Ω_t1(s)) = Ω_{t1×t2}(s)\n\nTwo rounds of tournament = one larger tournament\nPressures multiply", explanation: "Concatenation theorem: Two tournaments of size t1 and t2 equal one of size t1×t2. Pressures multiply.", keyPoints: ["Two rounds = one larger", "Pressures multiply"], formulas: ["tournamentConcatenation"] },
  { number: 228, title: "Reproduction Rate", session: 14, content: "Reproduction Rate: R(f) = ŝ(f)/s(f)\n\nR > 1: class grows\nR < 1: class shrinks\nR = 1: unchanged", explanation: "Reproduction rate R = after/before. R>1 grows, R<1 shrinks, R=1 unchanged.", keyPoints: ["R = after/before", "R>1: grows", "R<1: shrinks"], formulas: ["reproductionRate"] },
  { number: 229, title: "Tournament Reproduction Rate", session: 14, content: "For tournament:\nR depends only on relative position and t\nNot on absolute fitness values!", explanation: "Tournament R depends only on rank and t, NOT absolute values. This is why tournament is robust to scaling.", keyPoints: ["Depends on rank and t", "Not absolute values", "Why robust to scaling"] },
  { number: 230, title: "Selection Intensity", session: 14, content: "Selection Intensity: I = (⟨f⟩' - ⟨f⟩)/σ_f\n\nImprovement in standard deviations\nMeasures selection strength", explanation: "Selection intensity I = mean improvement / std dev. Measures selection strength in standard deviations.", keyPoints: ["I = improvement / σ", "Measures strength"], formulas: ["selectionIntensity"] },
  { number: 231, title: "Intensity Values", session: 14, content: "Typical I values:\nTournament k=2: I ≈ 0.56\nTournament k=5: I ≈ 1.15\nTruncation 10%: I ≈ 1.76", explanation: "Typical intensities: Tournament k=2 ≈ 0.56, k=5 ≈ 1.15. Truncation 10% ≈ 1.76 (very strong).", keyPoints: ["k=2: 0.56", "k=5: 1.15", "Truncation 10%: 1.76"] },
  { number: 232, title: "Loss of Diversity", session: 14, content: "Diversity loss: proportion not selected\n\nP_d = proportion with 0 copies\n\nHigher pressure = more loss", explanation: "Diversity loss = proportion never selected. Higher selection pressure = more individuals get 0 copies.", keyPoints: ["Proportion not selected", "Higher pressure = more loss"], formulas: ["lossOfDiversity"] },
  { number: 233, title: "Tournament Diversity Loss", session: 14, content: "Tournament: P_d = (proportion below median)^t\n\nAs t increases, more diversity lost\nBalance pressure vs diversity", explanation: "Tournament diversity loss: P_d = (below median)^t. Larger t = more loss. Must balance.", keyPoints: ["P_d = (below median)^t", "Larger t = more loss", "Must balance"] },
  { number: 234, title: "Takeover Time", session: 14, content: "Takeover time: Generations for best to fill population\n\nMeasures convergence speed\nDepends on selection pressure", explanation: "Takeover time = generations for single best to fill entire population. Measures convergence speed under selection only.", keyPoints: ["Generations to fill", "Convergence speed", "Depends on pressure"] },
  { number: 235, title: "Takeover Formula", session: 14, content: "Tournament takeover: τ* ≈ ln(N)/ln(k)\n\nSmall for large k (fast takeover)\nLarge for small k (slow takeover)", explanation: "Tournament takeover τ* ≈ ln(N)/ln(k). Large k = fast takeover. Small k = slow takeover.", keyPoints: ["τ* ≈ ln(N)/ln(k)", "Large k: fast", "Small k: slow"] },
  { number: 236, title: "Balancing Factors", session: 14, content: "Balance:\n- Fast convergence (high pressure)\n- Maintain diversity (low pressure)\n- Avoid premature convergence\n- Allow sufficient exploration", explanation: "Must balance: Fast convergence needs high pressure, diversity needs low pressure. Avoid premature convergence.", keyPoints: ["Fast vs diversity", "Avoid premature", "Allow exploration"] },
  { number: 237, title: "Parameter Guidelines", session: 14, content: "Tournament guidelines:\nStart with k=2 (binary)\nIncrease k for faster convergence\nDecrease k for more diversity", explanation: "Guidelines: Start k=2 (binary). Increase for speed. Decrease for diversity. Adjust based on performance.", keyPoints: ["Start k=2", "Increase: speed", "Decrease: diversity"] },
  { number: 238, title: "Selection and Population", session: 14, content: "Selection + Population Size:\nSmall pop + strong selection = premature\nLarge pop + weak selection = slow\n\nBoth parameters interact", explanation: "Selection and population interact. Small+strong = premature. Large+weak = slow. Both affect performance.", keyPoints: ["Interact together", "Small+strong: premature", "Large+weak: slow"] },
  { number: 239, title: "Adaptive Selection", session: 14, content: "Adaptive selection:\nStart weak (explore)\nIncrease over time (exploit)\n\nOr adjust based on diversity", explanation: "Adaptive selection: Start weak for exploration, strengthen over time for exploitation. Or adjust based on diversity metrics.", keyPoints: ["Start weak", "Strengthen over time", "Adjust by diversity"] },
  { number: 240, title: "Rank Selection Math", session: 14, content: "Linear ranking:\nP(rank r) = (2-s)/μ + 2(r-1)(s-1)/(μ(μ-1))\n\ns ∈ (1,2] controls pressure", explanation: "Linear ranking formula. Parameter s controls pressure. s=1: uniform, s=2: maximum linear.", keyPoints: ["Formula for linear ranking", "s controls pressure", "s=1: uniform, s=2: max"], formulas: ["linearRanking"] },
  { number: 241, title: "Comparing Selection Methods", session: 14, content: "Method comparison:\nFPS: Simple, problematic\nRank: Better, needs sorting\nTournament: Best overall\nTruncation: Fastest, risky", explanation: "Comparison: FPS simple/problematic, Rank better/sorting, Tournament best overall, Truncation fastest/risky.", keyPoints: ["FPS: problematic", "Rank: better", "Tournament: best", "Truncation: fast/risky"] },
  { number: 242, title: "Tournament Advantages Recap", session: 14, content: "Tournament advantages:\n1. Only comparisons needed\n2. O(k) per selection\n3. Not scaling sensitive\n4. Single parameter\n5. Parallelizable", explanation: "Tournament advantages: Only comparisons (correct fitness), O(k) complexity, not scaling sensitive, single parameter k, parallelizable.", keyPoints: ["Only comparisons", "O(k)", "Not scaling sensitive", "Single parameter", "Parallelizable"] },
  { number: 243, title: "Practical Selection", session: 14, content: "Practical recommendations:\n1. Default: Tournament k=2 to 7\n2. Use elitism\n3. Monitor diversity\n4. Adjust based on performance", explanation: "Practical: Default tournament k=2-7, use elitism, monitor diversity, adjust based on performance.", keyPoints: ["Default: tournament", "Use elitism", "Monitor diversity", "Adjust as needed"] },
  { number: 244, title: "Selection Summary Table", session: 14, content: "Summary:\nFPS: O(n), scaling sensitive\nRank: O(n log n), stable pressure\nTournament: O(k), robust, adjustable\nTruncation: O(n log n), very strong", explanation: "Summary table: FPS O(n)/sensitive, Rank O(n log n)/stable, Tournament O(k)/robust, Truncation O(n log n)/strong.", keyPoints: ["FPS: sensitive", "Rank: stable", "Tournament: robust", "Truncation: strong"] },
  { number: 245, title: "Key Mathematical Insights", session: 14, content: "Key insights:\n1. Selection transforms distribution\n2. Tournament: power transformation\n3. Pressure vs diversity tradeoff\n4. All methods have characteristic curves", explanation: "Key insights: Selection transforms fitness distribution, tournament uses power transformation, pressure-diversity tradeoff fundamental.", keyPoints: ["Transforms distribution", "Power transformation", "Pressure-diversity tradeoff"] },
  { number: 246, title: "Theory vs Practice", session: 14, content: "Theory vs Practice:\nTheory guides understanding\nPractice requires testing\nParameters problem-dependent\nNo universal best settings", explanation: "Theory guides, practice tests. Parameters problem-dependent. No universal settings—always tune.", keyPoints: ["Theory guides", "Practice tests", "Problem-dependent", "Always tune"] },
  { number: 247, title: "Course Key Insight", session: 14, content: "COURSE KEY INSIGHT:\nWe have CORRECT fitness (rankings)\nNot EXACT fitness (values)\n\nTournament selection exploits this!", explanation: "COURSE KEY INSIGHT: We have CORRECT fitness (rankings), NOT EXACT (values). Tournament selection exploits this—only needs comparisons.", keyPoints: ["CORRECT not EXACT fitness", "Rankings not values", "Tournament exploits this"], professorNote: "This is THE key insight of the course: We have correct fitness (rankings), not exact (values)." },
  { number: 248, title: "Course Summary", session: 14, content: "Course Summary:\n- EC models natural selection\n- Three operators: selection, crossover, mutation\n- Representation critical\n- Tournament selection robust\n- Always test empirically!", explanation: "Final summary: EC models natural selection. Three operators (selection, crossover, mutation). Representation critical. Tournament selection robust. Always test empirically!", keyPoints: ["EC models evolution", "Three operators", "Representation critical", "Tournament robust", "Test empirically!"] }
];

// Helper functions
export function getSlideByNumber(num: number): Slide | undefined {
  return SLIDES.find(s => s.number === num);
}

export function getSlidesBySession(sessionId: number): Slide[] {
  return SLIDES.filter(s => s.session === sessionId);
}

export function getSlideRange(start: number, end: number): Slide[] {
  return SLIDES.filter(s => s.number >= start && s.number <= end);
}

export function searchSlides(query: string): Slide[] {
  const q = query.toLowerCase();
  return SLIDES.filter(s => 
    s.title.toLowerCase().includes(q) ||
    s.content.toLowerCase().includes(q) ||
    s.explanation.toLowerCase().includes(q) ||
    s.keyPoints.some(kp => kp.toLowerCase().includes(q))
  );
}

export function getTotalSlides(): number {
  return SLIDES.length;
}
